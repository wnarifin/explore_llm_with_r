[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Large Language Models With R",
    "section": "",
    "text": "Preface\nThis book is my personal attempt to share my learning journey in implementing large language models (LLMs)1 within the R programming environment. There are not many tutorials available for this, so I hope this can serve as one of those resources. The content will be updated over several years as new developments occur and as I continue to learn more.\nThis book was written with the help of several of local LLMs, among which are Qwen, Llama, DeepSeek and Phi. Thanks buddies for your help (although I know they are inanimate). To ensure that I acknowledge their contributions, I have purposely included their names (denoted in between square brackets “[ LLM_NAME ]”) in some chapters and sections where the content relies largely on their generated texts. I used Quarto to prepare this book. If you want to learn more about Quarto books, please visit https://quarto.org/docs/books.\nI would like to thank the Ollama (https://ollama.com/) and Open WebUI (https://openwebui.com/) projects for their tremendous contributions in making local LLMs easily accessible.\nThank you for joining me on this adventure as we explore integrating LLMs into R. This is an evolving book that will grow with your feedback and contributions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Exploring Large Language Models With R",
    "section": "",
    "text": "There are also small, tiny and super tiny language models. Let’s simply call all the models that rely on the Transformer architecture as LLMs.↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "ch1.html",
    "href": "ch1.html",
    "title": "\n1  Large Language Models\n",
    "section": "",
    "text": "1.1 Introduction\nLarge language models (LLMs) are “foundation models trained on immense amounts of data making them capable of understanding and generating natural language and other types of content to perform a wide range of tasks” (IBM, 2024).\nThere are also small (Caballar, 2024), tiny (Tang et al., 2024), and super tiny (Hillier et al., 2024) language models. In this book, I refer to all language models that rely on the modern concept of Transformer-based model architecture as LLMs. When a nomenclature is based on numerical cutoff values in a rapidly expanding field of technology, sooner or later, what we know today as large will soon become hyper tiny. So, let’s simply call them LLMs.\nThis chapter’s content was primarily generated using local LLMs. I would appreciate any corrections regarding the information presented.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch1.html#background",
    "href": "ch1.html#background",
    "title": "\n1  Large Language Models\n",
    "section": "\n1.2 Background",
    "text": "1.2 Background\n[Qwen2.5:14b] [Phi4:14b]\nThe rise of Large Language Models represents a significant leap forward in artificial intelligence and language understanding. These advanced computer systems are designed to understand and generate human-like text proficiently. Characterised by their extensive size—referring not only to the volume of data they can process but also to the complexity and depth of their neural network architecture—the “large” in LLM signifies that these models consist of billions or even trillions of parameters, enabling them to capture nuanced patterns within vast datasets.\nThe development of LLMs is rooted in advancements in machine learning, particularly deep learning techniques. These techniques involve training artificial neural networks on massive amounts of text data, allowing the models to learn statistical relationships between words, phrases, sentences, and entire documents. This enables them to perform a variety of language tasks such as translation, summarization, question answering, and content generation.\nA key feature of LLMs is their ability to generalise from training data to new, unseen inputs. For example, after being trained on diverse text sources, an LLM can generate coherent and contextually appropriate responses in conversations or provide insightful answers to questions it was not explicitly programmed to handle. This generative capability makes them particularly useful for applications like chatbots, virtual assistants, and creative writing aids.\nThe architecture of LLMs often relies on transformer models, which are designed to process sequences of data efficiently. Transformers utilise mechanisms such as attention layers that allow the model to focus on different parts of an input sequence dynamically, improving its ability to understand context and meaning in complex sentences. This design is crucial for handling tasks requiring long-term dependency understanding, a common challenge in natural language processing.\nAs LLMs continue to evolve, they are becoming increasingly powerful tools across various industries. In healthcare, they assist with medical documentation by summarising patient notes or providing information retrieval. In education, they help generate practice exercises and personalised learning materials. Furthermore, their ability to process and synthesise large volumes of text has significant implications for content creation and curation in media.\nDespite these capabilities, LLMs also pose challenges and ethical considerations. Concerns about data privacy, potential biases embedded in the training data, and the environmental impact of their extensive computational requirements are ongoing discussions within the AI community. Additionally, ensuring that these models align with human values and operate safely in diverse contexts remains a critical area for research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch1.html#brief-history",
    "href": "ch1.html#brief-history",
    "title": "\n1  Large Language Models\n",
    "section": "\n1.3 Brief History",
    "text": "1.3 Brief History\n[Qwen2.5:14b] [DeepSeek-R1:14b]\nThe development of Large Language Models has been driven by significant advances in artificial intelligence and deep learning over the past four decades. The journey began with simpler models that focused on understanding basic language patterns, but it quickly evolved into sophisticated systems capable of handling complex linguistic tasks.\n\n1.3.1 Early Beginnings: The 1980s-1990s\n\n\nNeural Networks Emerge: The exploration of neural networks for natural language processing (NLP) began in the mid-to-late 1980s. These early models laid the groundwork for connectionist approaches, moving away from rule-based systems.\n\nRecurrent Neural Networks (RNNs): By the late 1980s and early 1990s, RNNs started being applied to language modeling, marking a significant shift in NLP techniques.\n\n1.3.2 The Advent of LSTM and Deep Learning: Early 2000s-2010s\n\n\nLSTM Networks: In the mid-2000s, Long Short-Term Memory (LSTM) networks were introduced by Hochreiter and Schmidhuber in 1997. These networks improved RNN performance by effectively capturing long-range dependencies.\n\nBreakthrough with Deep Learning: The early 2010s saw advancements with models like those developed at Facebook AI Research (FAIR), enhancing the capabilities of neural networks through deep learning techniques.\n\n1.3.3 Transformer Architecture and Pre-training: Late 2010s\n\n\nTransformer Introduction: The Transformer architecture, introduced in a 2017 paper by Vaswani et al. (2017), revolutionized NLP with its self-attention mechanism. Since then, Transformers became widely adopted, leading to significant advancements in language modeling.\n\nPre-trained Models: In 2017, BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer) emerged, utilizing unsupervised learning methods to achieve state-of-the-art performance.\n\n1.3.4 Scaling Up: The Era of Large Language Models\n\n\nGPT-3 and Beyond: In 2020, OpenAI’s GPT-3, trained on massive datasets, demonstrated impressive capabilities, highlighting the potential of large-scale models.\n\nRecent Advancements: Current research focuses on improving efficiency, ethics, and applications, with ongoing developments in models like T5, PaLM, and others.\n\nIn summary, the evolution of large language models began in the late 20th century with neural networks replacing rule-based NLP systems. Recurrent Neural Networks (RNNs) and LSTM networks emerged, enhancing language modeling. The Transformer architecture, introduced in 2016, revolutionized NLP through self-attention mechanisms. Pre-trained models like BERT and GPT advanced unsupervised learning, leading to the development of large-scale models such as GPT-3, marking a new era in AI-driven language processing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch1.html#the-transformer-model",
    "href": "ch1.html#the-transformer-model",
    "title": "\n1  Large Language Models\n",
    "section": "\n1.4 The Transformer Model",
    "text": "1.4 The Transformer Model\n[Phi4:14b]\nThe Transformer model, introduced by Vaswani et al. (2017) in their seminal 2017 paper “Attention is All You Need,” has become a cornerstone for advancements in natural language processing (NLP). It marked a significant departure from traditional sequence-based models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs), which process data sequentially. Instead, Transformers leverage parallelization to handle entire sequences at once, making them more efficient and scalable.\n\n1.4.1 Key Components of the Transformer Model\n\n\nThe Transformer model architecture. Source: Vaswani et al. (2017).\n\n\n\nSelf-Attention Mechanism:\n\nAt the heart of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different words in a sentence relative to each other.\nThis mechanism computes attention scores for every word pair in a sequence, enabling the model to capture dependencies regardless of their distance from each other.\n\n\n\nEncoder and Decoder Architecture:\n\nThe Transformer architecture consists of an encoder and a decoder, both composed of multiple layers.\nThe encoder processes the input sequence and converts it into continuous representations that capture its meaning.\nThe decoder then generates the output sequence by attending to these encoded representations and the partially generated output.\n\n\n\nPositional Encoding:\n\nSince Transformers do not inherently process sequences in order, they use positional encodings to inject information about the position of each word in a sentence.\nThis allows the model to maintain an understanding of word order, which is crucial for meaningful language processing.\n\n\n\nLayer Normalization and Feed-Forward Networks:\n\nEach layer in both the encoder and decoder includes sub-layers such as multi-head self-attention and position-wise feed-forward networks, followed by normalization.\nThese components ensure that the model can learn complex patterns while maintaining stability during training.\n\n\n\n1.4.2 Advantages of Transformers\n\nParallelization: Unlike RNNs, which process sequences step-by-step, Transformers handle entire sequences simultaneously. This parallelism significantly speeds up training and inference times.\nScalability: The architecture’s ability to leverage large datasets and compute resources has enabled the development of models like GPT (Generative Pre-trained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), which have set new benchmarks in various NLP tasks.\nFlexibility: Transformers can be adapted for a wide range of applications beyond translation, including text summarization, sentiment analysis, and more.\n\n1.4.3 Impact on Large Language Models\nThe introduction of the Transformer model has been pivotal in the evolution of LLMs. By enabling efficient training on massive datasets and facilitating transfer learning, Transformers have empowered models to achieve unprecedented levels of linguistic understanding and generative capability. This has led to applications ranging from conversational agents to sophisticated content generation tools.\nThe Transformer model’s innovative architecture and attention mechanism have revolutionized NLP, laying the groundwork for the development of powerful large language models that continue to push the boundaries of what machines can understand and generate in human languages.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch1.html#recent-applications-of-large-language-models",
    "href": "ch1.html#recent-applications-of-large-language-models",
    "title": "\n1  Large Language Models\n",
    "section": "\n1.5 Recent Applications of Large Language Models",
    "text": "1.5 Recent Applications of Large Language Models\n[Qwen2.5:14b] [Phi4:14b]\nLarge Language Models have seen significant advancements and are being applied across diverse domains, including healthcare. These models leverage their ability to understand and generate human-like text to solve complex problems efficiently.\n\n1.5.1 General Applications\nAutomated Content Generation: LLMs are adept at transforming structured data into human-readable content, which is invaluable for businesses and organizations needing to produce large volumes of text efficiently. They can automatically generate reports by analyzing datasets and identifying key trends or insights. In journalism, they assist in producing news articles by summarizing events from structured inputs like press releases or databases. Similarly, LLMs can create executive summaries that highlight the most important points from detailed documents, saving time for professionals who need to digest large amounts of information quickly.\nChatbots and Virtual Assistants: LLMs serve as the backbone for advanced chatbots and virtual assistants, providing natural language understanding and generation capabilities. These models can handle a wide range of queries, offering customer support by answering frequently asked questions or resolving issues based on pre-defined knowledge bases. Moreover, they personalize interactions by learning user preferences over time, thus enhancing user satisfaction. In e-commerce, for example, LLM-powered chatbots can recommend products tailored to individual shopping habits, while in healthcare, virtual assistants might provide initial triage assessments based on patient inputs.\nCode Completion and Debugging: Software development benefits significantly from LLMs through code completion features that predict what a developer intends to write next, enhancing productivity by reducing manual coding effort. These models analyze existing codebases to suggest logical continuations or improvements, effectively serving as an intelligent pair programmer. Additionally, LLMs can assist in debugging by identifying potential errors and suggesting corrections based on patterns observed in similar code snippets. This capability helps developers quickly resolve issues and maintain cleaner, more efficient code.\nText Summarization and Translation: LLMs excel at condensing lengthy documents into concise summaries, capturing essential information without losing context or meaning. This functionality is crucial for professionals who need to review extensive reports or research papers efficiently. In translation tasks, LLMs leverage their understanding of multiple languages to provide accurate translations while maintaining the original text’s tone and style. They can handle complex language nuances and idiomatic expressions better than traditional machine translation systems, facilitating effective cross-cultural communication in globalized environments.\n\n1.5.2 Academic Applications\nResearch Assistance: LLMs significantly enhance research efficiency by performing tasks like summarizing lengthy academic papers into digestible abstracts, thereby saving researchers valuable time. They can also generate comprehensive literature reviews by synthesizing information from multiple sources, highlighting trends and gaps within a field of study. Furthermore, LLMs suggest relevant studies or articles that might not be immediately apparent through traditional search methods, ensuring thoroughness in research projects.\nData Analysis: In disciplines such as social sciences and bioinformatics, analyzing qualitative data can be challenging due to the sheer volume and complexity of text-based information. LLMs assist by identifying patterns, themes, or significant correlations within these large datasets, providing researchers with deeper insights that might be missed through manual analysis. This capability enhances both the depth and efficiency of research analyses, allowing for more robust conclusions.\nEducational Tools: LLMs integrated into educational platforms revolutionize personalized learning experiences. They generate practice problems tailored to individual student needs and provide instant feedback on student work, which helps reinforce learning objectives. Additionally, LLMs can design adaptive learning paths that evolve based on a student’s progress, ensuring that each learner receives the appropriate level of challenge and support.\nGrant Writing and Proposal Development: Academics often face challenges in crafting compelling grant proposals due to the competitive nature of funding opportunities. LLMs assist by providing structural templates, suggesting content improvements, or generating sections of the proposal text. This not only enhances productivity but also increases the quality and coherence of proposals, making them more likely to succeed.\nLanguage Translation and Interpretation: LLMs with multilingual capabilities bridge language barriers in academic collaboration, enabling seamless translation of documents and interpretation of research findings across languages. This functionality is crucial for international collaborations, allowing researchers from diverse linguistic backgrounds to share insights without the constraints of language limitations.\nPeer Review Processes: Institutions are exploring how LLMs can support peer review by offering initial assessments of manuscripts. These models evaluate aspects like clarity, coherence, and potential issues, helping reviewers focus on more critical content evaluations rather than basic structural concerns. This streamlines the review process while maintaining rigorous academic standards.\nSimulated Dialogues for Training: LLMs are valuable tools in training students within psychology and education fields by simulating human dialogues. These simulations allow learners to practice conversational skills or therapeutic techniques in a safe, controlled environment, providing an opportunity to refine their abilities before real-world application.\n\n1.5.3 Medical Applications\nClinical Documentation: LLMs significantly reduce the administrative burden on healthcare providers by automating clinical documentation tasks. AI-driven systems can generate detailed patient progress notes from voice recordings during consultations or directly from electronic health records (EHRs). This automation allows clinicians to focus more on patient care rather than paperwork, improving efficiency and accuracy in maintaining comprehensive medical records.\nDisease Diagnosis and Prediction: LLMs enhance diagnostic processes by analyzing vast amounts of medical literature and patient data to identify disease patterns. These models can suggest potential diagnoses based on a combination of symptoms, patient history, and known medical correlations. Additionally, LLMs aid in predicting the progression of diseases, allowing healthcare providers to implement early interventions or personalized treatment plans that could improve patient outcomes.\nPatient Education Materials: LLMs generate personalized educational materials for patients by tailoring content based on individual medical records and demographic information. This ensures that patients receive clear, understandable explanations about their conditions and available treatment options. Customized pamphlets or web pages can address specific concerns or questions, empowering patients to actively participate in their own healthcare management.\nPharmaceutical Research: In the realm of drug discovery and development, LLMs analyze extensive scientific literature to identify potential targets and compounds. By scanning databases of chemical structures and biological pathways, these models suggest promising drug candidates for further investigation. This capability accelerates the research process, potentially leading to faster development of new medications and therapies.\nMental Health Support: LLMs power virtual therapists that provide empathetic support for mental health issues. These conversational agents offer advice and resources based on user input, assisting with managing conditions such as anxiety or depression. By simulating human-like interactions, LLMs can offer immediate support and guidance, supplementing traditional therapy options and increasing accessibility to mental health care.\nMedical Record Management: LLMs facilitate the organization and retrieval of medical records by enabling intelligent search capabilities across large datasets. Healthcare professionals can efficiently access relevant patient history, lab results, and previous treatments with these tools, improving continuity of care and ensuring that critical information is readily available when needed.\nResearch Literature Summarization: LLMs help clinicians and researchers stay informed about the latest advancements by summarizing recent research papers and studies. Automated summaries provide concise overviews of complex journal articles in fields such as oncology, cardiology, or neurology. This application saves time for healthcare professionals, allowing them to quickly assimilate new knowledge and integrate it into their practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch1.html#references",
    "href": "ch1.html#references",
    "title": "\n1  Large Language Models\n",
    "section": "References",
    "text": "References\n\n\n\n\nCaballar, R. (2024). What are small language models? Website. Retrieved from https://www.ibm.com/think/topics/small-language-models\n\n\nHillier, D., Guertler, L., Tan, C., Agrawal, P., Ruirui, C., & Cheng, B. (2024). Super tiny language models. Retrieved from https://arxiv.org/abs/2405.14159\n\n\nIBM. (2024). What are large language models (LLMs)? Website. Retrieved from https://www.ibm.com/think/topics/large-language-models\n\n\nTang, Y., Liu, F., Ni, Y., Tian, Y., Bai, Z., Hu, Y.-Q., … Wang, Y. (2024). Rethinking optimization and architecture for tiny language models. Retrieved from https://arxiv.org/abs/2402.02791\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in neural information processing systems (Vol. 30). Curran Associates, Inc. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch2.html",
    "href": "ch2.html",
    "title": "\n2  Local Large Language Models\n",
    "section": "",
    "text": "2.1 Introduction\nLocal large language models (LLMs) “are versions of these powerful language models that run directly on a user’s device or local network, rather than in the cloud” (\"Unleashing the Power of Local LLMs\", 2024). With the availability of relatively powerful consumer-grade GPUs with reasonably sized VRAM, we can run many open-weight LLM models locally. We will go through the reasons, requirements and options of running LLMs locally, and how to run it in R.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Local Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch2.html#reasons-of-using-local-llms",
    "href": "ch2.html#reasons-of-using-local-llms",
    "title": "\n2  Local Large Language Models\n",
    "section": "\n2.2 Reasons of Using Local LLMs",
    "text": "2.2 Reasons of Using Local LLMs\n\n2.2.1 Exhibits\nThese are some of my main concerns about relying too much on any LLM services online, which can be illustrated clearly in these two pictures:\n\n\nSource: https://status.openai.com\n\n\n\nSource: https://chatgpt.com/ when you ran out of limit on free account.\n\n\n2.2.2 Advantages\nLocal LLMs offer five key advantages (\"Unleashing the Power of Local LLMs\", 2024) that make them a compelling choice for various applications.\nPrivacy is a major benefit, as these models process data entirely on your device, ensuring that sensitive information never leaves your control. Additionally, local LLMs provide reduced latency and offline functionality, making them ideal for scenarios where internet connectivity is unreliable or unnecessary. Without the need for constant cloud computing, users can enjoy faster response times and leverage AI capabilities even when disconnected from the web.\nUsing local LLMs is also cost-effective, as it eliminates ongoing expenses associated with cloud services, particularly for heavy users. Lastly, local deployment allows for greater customization, enabling fine-tuning of models to specific domains or use cases to meet precise needs.\n\n2.2.3 Personal reasons\n\nPrivacy: As a university lecturer, some tasks are sensitive in nature, such as writing exam questions, brainstorming novel ideas, and drafting top-secret research. Therefore, privacy is crucial. Local LLMs ensure that all data remains on my device, keeping the data private.\nNo Downtime: With local LLMs, I can work with them as long as my PC is running. I can work even if the internet goes down. This reliability ensures that I can focus entirely on my work.\nExperimentation: As a researcher, I love the freedom to experiment and iterate freely. Local LLMs provide this flexibility by allowing me to experiment with different models and explore different settings without worrying about whether I have reached my token limit of the day! As of today, local LLMs offer so much, and it is exciting to explore what they can do.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Local Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch2.html#using-local-llms",
    "href": "ch2.html#using-local-llms",
    "title": "\n2  Local Large Language Models\n",
    "section": "\n2.3 Using Local LLMs",
    "text": "2.3 Using Local LLMs\n\n2.3.1 Hardware Requirements\nFor starter, you’ll need a gaming-specs PC/Laptop with an NVIDIA GPU.\n\n\nSource: https://www.nvidia.com/en-us/geforce/graphics-cards/\n\nAs a disclaimer, I am not affiliated with NVIDIA, although I specifically mention NVIDIA GPUs here. As we will see later, packages/software for running local LLMs support NVIDIA GPUs, while the support for other GPUs may vary.\n\n\nSource: Generated with Gemini’s Imagen 3\n\nAnd, of course, it is cool to have this one (i.e. a gaming PC) sitting on your desk for the sake of research. Admittedly, this is a gaming-specs PC, so it’s up to you what you want to do with it.\n\n2.3.2 Options for Running Local LLMs\nThere are many options to run local LLMs, some of them are:\n\nOllama (https://ollama.com/) \n\nOllama + Open WebUI (https://openwebui.com/) \n\nMsty (https://msty.app/)\nLM Studio (https://lmstudio.ai/)\nGPT4All (https://www.nomic.ai/gpt4all)\nvLLM (https://docs.vllm.ai/)\nllama.cpp (https://github.com/ggerganov/llama.cpp) – essentially the originator of all listed above.\n\nIn this book, we will use Ollama as the primary driver for running local LLMs and integrate it with our beloved R.\n\n\nOllama main page https://ollama.com/\n\nAs it is, Ollama is run in CLI,\n\n\nOllama CLI shown running llama3.2\n\nFor daily use, Ollama is typically combined with a GUI, such as Open WebUI,\n\n\nOllama in Open WebUI",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Local Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch2.html#running-local-llms-in-r",
    "href": "ch2.html#running-local-llms-in-r",
    "title": "\n2  Local Large Language Models\n",
    "section": "\n2.4 Running Local LLMs in R",
    "text": "2.4 Running Local LLMs in R\n\n2.4.1 Ollama Meets R\nThere are several options that utilizes Ollama:\n\n\nrollama  (https://jbgruber.github.io/rollama/)\n\nollamar (https://hauselin.github.io/ollama-r/)\n\nmall (https://mlverse.github.io/mall/)\n\nThere are also several other options that allow tapping into other LLM APIs other than Ollama:\n\n\ngptstudio (https://michelnivard.github.io/gptstudio/) and gpttools (https://jameshwade.github.io/gpttools/)\n\ntidyllm (https://edubruell.github.io/tidyllm/)\n\nellmer (https://github.com/tidyverse/ellmer/)\n\n2.4.2 Getting Started with rollama\n\nIn this book, we will mainly rely on rollama package (Gruber & Weber, 2025). To get started, there are three basic preliminary steps:\n\nInstall Ollama https://ollama.com/download (this depends on the OS in your PC)\nInstall rollama package in R with install.packages(\"rollama\")\n\nFind suitable models in https://ollama.com/search\n\n\n\n\nLLM models in ollama.com\n\nAvailable LLM models in Ollama are divided by tasks that they perform:\n\n\nText to Text – Text generation, i.e. text in / text out (a typical LLM)\n\nExamples: Llama (Meta, https://www.llama.com/), Qwen (Alibaba, https://qwenlm.github.io/gemm), DeepSeek (DeepSeek, https://www.deepseek.com/), Gemma (Google, https://ai.google.dev/gemma), Mistral (Mistral AI, https://mistral.ai/) and Phi (Microsoft, https://azure.microsoft.com/en-us/products/phi).\n\n\nText LLM / Tools in ollama.com\n\n\n\nImage + Text to Text – Vision LM, i.e. image + text in / text out\n\nExamples: LLaVA (https://llava-vl.github.io/), Llama3.2-Vision (Meta, https://www.llama.com/), Moondream (Moondream AI, https://moondream.ai/)and MiniCPM-V (ModelBest, https://modelbest.cn/en).\n\n\nVision LM in ollama.com\n\n\n\nText to Number – Embedding generation, i.e. text in / numerical vector out\n\nExamples: Nomic Embed (Nomic AI https://www.nomic.ai/) and mxbai-embed-large (Mixedbread, https://www.mixedbread.ai/).\n\n\nEmbedding models in ollama.com\n\n\n2.4.3 Basic rollama Usage\nInstall models\nYou can pull models from Ollama with pull_model(). Let’s pull llama3.2, moondream, and nomic-embed-text for starter,\n\nlibrary(rollama)\npull_model(\"llama3.2\")\npull_model(\"moondream\")\npull_model(\"nomic-embed-text\")\n\nList installed Models\nYou can list installed models in Ollama,\n\nlist_models()\n\n\n  \n\n\n\nQuery\nquery() is used when you want to ask a one-off query. The LLM will not remember the previous query.\n\nq = \"Describe the history of R programming language\"\nquery(q, \"llama3.2\")\n\n\n\n\n── Answer from llama3.2 ────────────────────────────────────────────────────────\n\n\nThe R programming language was created in the mid-1990s by Ross Ihaka and\nRobert Gentleman at the University of Auckland, New Zealand. Here's a brief\nhistory of R:\n\n\n\n\n\n**Early Days (1992-1995)**\n\n\n\n\n\nRoss Ihaka and Robert Gentleman were both statisticians and researchers who\nwanted to create a software package for statistical analysis that was easy to\nuse, flexible, and accessible. They began working on the project in 1992 and\nstarted calling it \"S\", after the programming language S-PLUS.\n\n\n\n\n\n**Release of S (1995)**\n\n\n\n\n\nThe first release of S, version 3.0, was made available in 1995. This initial\nversion was a significant improvement over earlier versions but still had some\nlimitations.\n\n\n\n\n\n**Development of R (1997-2001)**\n\n\n\n\n\nIn 1997, Ihaka and Gentleman decided to create a new project called \"R\", which\nstood for \"Rehabilitation\" or \"Revitalization\" of the S language. They wanted\nto make the software more user-friendly and expand its capabilities.\n\n\n\n\n\n**Release of R 0.5 (1998)**\n\n\n\n\n\nThe first release of R, version 0.5, was made available in 1998. This version\nintroduced several key features, including support for plotting, data\nvisualization, and statistical modeling.\n\n\n\n\n\n**Community Support (2001-2004)**\n\n\n\n\n\nIn 2001, the R project gained momentum with the creation of a mailing list,\nwhich allowed users to ask questions, share ideas, and collaborate on\ndevelopment. The R community grew rapidly, and by 2004, there were hundreds of\ncontributors to the project.\n\n\n\n\n\n**Release of R 2.0 (2005)**\n\n\n\n\n\nThe release of R 2.0 in 2005 marked a significant milestone for the language.\nThis version introduced several new features, including support for\nobject-oriented programming, improved data visualization, and enhanced\nstatistical modeling capabilities.\n\n\n\n\n\n**R Foundation Established (2006)**\n\n\n\n\n\nIn 2006, the R Foundation for Statistical Computing was established to manage\nand coordinate the development of R. The foundation has played a crucial role\nin promoting the language and providing support to users worldwide.\n\n\n\n\n\n**Release of R 3.0 (2012)**\n\n\n\n\n\nThe release of R 3.0 in 2012 marked another major milestone for the language.\nThis version introduced several key features, including improved data\nvisualization, enhanced statistical modeling capabilities, and significant\nperformance improvements.\n\n\n\n\n\n**Current State (2020-Present)**\n\n\n\n\n\nToday, R is one of the most popular programming languages used in data science\nand statistical analysis. The language has a thriving community of developers,\nusers, and contributors, with thousands of packages available for various\ntasks, from data visualization to machine learning.\n\n\n\n\n\nOverall, the history of R is marked by its commitment to simplicity,\nflexibility, and accessibility. From its humble beginnings as S to its current\nstatus as a leading data science language, R has become an indispensable tool\nfor statisticians, researchers, and data scientists around the world.\n\n\nChat\nchat() is used when you want to ask several consecutive queries. The LLM will remember the previous queries. This is the behaviour of chat LLMs.\n\nq = \"Describe the history of R programming language\"\nchat(q, \"llama3.2\")\n\n\n\n\n── Answer from llama3.2 ────────────────────────────────────────────────────────\n\n\nThe R programming language has a rich and fascinating history that spans over\nthree decades. Here's an overview:\n\n\n\n\n\n**Early Beginnings (1980s)**\n\n\n\n\n\nR was first conceived in the early 1980s by Ross Ihaka, a statistician at the\nUniversity of Auckland in New Zealand. Ihaka began working on a statistical\nanalysis system for his own research needs, which eventually evolved into what\nwould become the R programming language.\n\n\n\n\n\n**The First Release (1993)**\n\n\n\n\n\nIn 1993, Ihaka and Robert Gentleman, another statistician at Bell Labs in the\nUnited States, released the first version of R, called \"S-PLUS\" or \"R-1.\" This\ninitial release was based on the S programming language, which was developed by\nJohn Chambers at AT&T's Bell Labs.\n\n\n\n\n\n**The Name Change (1995)**\n\n\n\n\n\nIn 1995, Ihaka and Gentleman decided to rename the system from \"S-PLUS\" to\nsimply \"R,\" reportedly because they didn't want to associate it with the\ncommercial software S-PLUS.\n\n\n\n\n\n**The R Development Team Forms (1996)**\n\n\n\n\n\nAs R gained popularity, a group of developers formed to continue its\ndevelopment. The R Development Core Team was established in 1996, and it has\nsince been responsible for maintaining and updating the language.\n\n\n\n\n\n**R 1.0 Released (2003)**\n\n\n\n\n\nIn 2003, the first official version of R, R 1.0, was released. This marked a\nsignificant milestone in the language's development, as it introduced many new\nfeatures and improvements.\n\n\n\n\n\n**The CRAN (2002)**\n\n\n\n\n\nAround the same time, the Comprehensive R Archive Network (CRAN) was\nestablished in 2002. CRAN is now the official repository for R packages, where\nusers can upload and share their own software components.\n\n\n\n\n\n**R Packages Ecosystem (2000s)**\n\n\n\n\n\nIn the mid-2000s, the R ecosystem began to grow rapidly, with the creation of\nnumerous package repositories like CRAN and Bioconductor. This led to a vast\narray of tools and libraries available for data analysis and visualization in\nR.\n\n\n\n\n\n**Recent Developments (2010s-present)**\n\n\n\n\n\nIn recent years, R has continued to evolve with new features, such as:\n\n\n\n\n\n1. **Rcpp**: A bridge between R and C++, allowing developers to integrate C++\ncode into their R packages.\n\n\n2. **RShiny**: An interactive web application framework for creating\ndata-driven websites.\n\n\n3. **dplyr**: A popular package for efficient data manipulation and analysis.\n\n\n\n\n\n**The Future of R**\n\n\n\n\n\nToday, R is one of the most widely used programming languages in academia and\nindustry, particularly in the fields of statistics, data science, and machine\nlearning. The language's continued growth and development are driven by a\nvibrant community of users and developers who contribute to its ecosystem\nthrough new packages, plugins, and extensions.\n\n\n\n\n\nOverall, the history of R is a story of collaborative innovation, with\ncontributions from many individuals over several decades. Its enduring\npopularity is a testament to its flexibility, power, and versatility as a tool\nfor data analysis and visualization.\n\n\nContinue with the chat,\n\nchat(\"Summarize it\", \"llama3.2\")\n\n\n\n\n── Answer from llama3.2 ────────────────────────────────────────────────────────\n\n\nHere's a brief summary:\n\n\n\n\n\nR programming language was first conceived in the 1980s by Ross Ihaka at the\nUniversity of Auckland. The first version, R-1, was released in 1993, but it\nwasn't until 2003 that the official version R 1.0 was released. In the\nmid-2000s, the Comprehensive R Archive Network (CRAN) was established, and the\necosystem began to grow rapidly.\n\n\n\n\n\nIn recent years, R has continued to evolve with new features like Rcpp, RShiny,\nand dplyr. Today, R is a widely used language in academia and industry,\nparticularly in statistics, data science, and machine learning. Its enduring\npopularity is driven by a vibrant community of users and developers who\ncontribute to its ecosystem.\n\n\n\n\n\nKey milestones:\n\n\n\n\n\n* 1980s: Ross Ihaka conceives the idea for R\n\n\n* 1993: First version of R released (R-1)\n\n\n* 2003: Official version R 1.0 released\n\n\n* 2002: Comprehensive R Archive Network (CRAN) established\n\n\n* Mid-2000s: R ecosystem grows rapidly with new package repositories and tools\n\n\n\n2.4.4 LLM Model Details\nView the details\nImportant details of LLM models in your Ollama,\n\nlist_models() |&gt; names()\n\n [1] \"name\"               \"model\"              \"modified_at\"       \n [4] \"size\"               \"digest\"             \"parent_model\"      \n [7] \"format\"             \"family\"             \"families\"          \n[10] \"parameter_size\"     \"quantization_level\"\n\n\n\nlist_models()[, c(\"name\", \"family\", \"parameter_size\", \"format\", \"quantization_level\")]\n\n\n  \n\n\n\n\n2.4.5 Terms to understand\nModel size\n\nIt usually described as the number of parameters in billions (B)\nParameters = weights in deep neural networks\nLarger = better, but heavy to run (massive GPU requirement)\nSmaller = (maybe) not as good, but lighter to run (consumer GPU can run)\nGenerally:\n\n4B model = GPU 4Gb VRAM\n8B model = GPU 8Gb VRAM\n16B model = GPU 16Gb VRAM\n\n\n\nQuantization\n\nIt is a technique “to reduce the model’s memory footprint and computational requirements without significantly sacrificing performance”.\nThe process reduces the precision of the weights of LLM models.\nFloating points: FP32, FP16; integers: Q2, Q4 (common, default in ollama), Q5, Q6, Q8. in bits, i.e. Q4 is 4-bit integer.\nQuantization allows us to run local LLMs in our consumer grade PC.\nMore on quantization at https://huggingface.co/docs/hub/gguf#quantization-types, and this Youtube video by Matt Williams that explains it very well https://youtu.be/K75j8MkwgJ0?si=W3KBSRJPlI0QpMxr.\n\nContext size\n\nContext window/size is number of tokens (words or sub-words) that can be LLM can receive/produce as input/output. - It is around 3/2 times words in a given text.\nYou can try a context size calculator here: https://llmtokencounter.com/\n\nThis Youtube video by Matt Williams really explains the concept very well https://youtu.be/-Lyk7ygQw2E?si=RJwx9Xpl80MIDnuF.\n\n2.4.6 Pulling Additional Models from Huggingface\n\nPull GGUF files from https://huggingface.co/models for use in Ollama\nYou may find GGUF files for Malaysian LLM models (mostly fine-tuned and developed by mesolitica https://mesolitica.com/)\n\n\n\nmesolitica GGUF files\n\n\n\nmalaysian GGUF files",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Local Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch2.html#references",
    "href": "ch2.html#references",
    "title": "\n2  Local Large Language Models\n",
    "section": "References",
    "text": "References\n\n\n\n\nGruber, J. B., & Weber, M. (2025). Rollama: Communicate with ollama to run large language models locally. Retrieved from https://jbgruber.github.io/rollama/\n\n\n\"Unleashing the Power of Local LLMs\". (2024). Unleashing the power of local LLMs: A comprehensive guide. Website. Retrieved from https://localxpose.io/blog/unleashing-the-power-of-local-llms",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Local Large Language Models</span>"
    ]
  },
  {
    "objectID": "ch3.html",
    "href": "ch3.html",
    "title": "\n3  Text Generation\n",
    "section": "",
    "text": "3.1 Preliminaries\nOne of the primary uses of Large Language Models (LLMs) is their ability to generate high-quality textual content, making them invaluable in a variety of applications. Common examples include automated content creation for blogs and social media posts, intelligent chatbots that engage in natural conversations, and multilingual translations that maintain accuracy and fluency. These models can produce coherent narratives, informative summaries, and conversational responses that closely mimic human writing styles.\nIn this chapter, we will explore how to perform text generation tasks in R using the rollama package (Gruber & Weber, 2025) for a variety of relevant use cases in academic and medical research. For the generated text, you will actually get different results from mine due to the random nature of text generation process.\nLoad these packages,\nlibrary(rollama)\nlibrary(purrr)\nlibrary(tibble)\nMake sure that your Ollama is running,\nping_ollama()  # ensure it's running\n\n▶ Ollama (v0.5.1) is running at &lt;http://localhost:11434&gt;!\n\n# list_models()$name #&gt; run this to view available models in your system\nGive names to our buddies,\nllama = \"llama3.2:3b\"\nmalay = \"hf.co/RichardErkhov/mesolitica_-_malaysian-llama-3-8b-instruct-16k-gguf:Q4_K_M\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#interpreting-statistical-results",
    "href": "ch3.html#interpreting-statistical-results",
    "title": "\n3  Text Generation\n",
    "section": "\n3.2 Interpreting Statistical Results",
    "text": "3.2 Interpreting Statistical Results\nRun a linear regression using the multi-purpose dataset mtcars,\n\nlm_model = lm(mpg ~ wt, data = mtcars)\nsummary(lm_model)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5432 -2.3647 -0.1252  1.4096  6.8727 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  37.2851     1.8776  19.858  &lt; 2e-16 ***\nwt           -5.3445     0.5591  -9.559 1.29e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.046 on 30 degrees of freedom\nMultiple R-squared:  0.7528,    Adjusted R-squared:  0.7446 \nF-statistic: 91.38 on 1 and 30 DF,  p-value: 1.294e-10\n\n\nSave the output into a suitable text format,\n\ntext_data = summary(lm_model) |&gt; capture.output() |&gt; paste(collapse = \"\\n\")\n\nPut everything together nicely in a tibble,\n\nq_text = tribble(\n  ~role,    ~content,\n  \"system\", \"Interpret the statistical results from the given text:\",\n  \"user\",   text_data\n)\nq_text\n\n\n  \n\n\n\nQuery to interpret the statistical result,\n\nquery(q_text, llama, output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\n**Regression Analysis Results**\n\nThe statistical results from the given text are for a simple linear regression analysis of `mpg` (miles per gallon) in the `mtcars` dataset as a function of `wt` (weight). Here's an interpretation of the key statistics:\n\n**Residuals:**\n\n* The residual plot shows that there is a slight upward trend, indicating some non-linear relationships or outliers that may affect the model's accuracy.\n* The residuals are approximately normally distributed, with most values clustering around 0.\n\n**Coefficients:**\n\n* **Intercept (Constant):**\n    + Estimated value: 37.2851\n    + Standard error: 1.8776\n    + t-value: 19.858 (highly significant)\n    + p-value: &lt;2e-16 (very strong evidence against the null hypothesis that the intercept is 0)\n* **Slope (Weight):**\n    + Estimated value: -5.3445\n    + Standard error: 0.5591\n    + t-value: -9.559 (highly significant)\n    + p-value: 1.29e-10 (strong evidence against the null hypothesis that the slope is 0)\n\n**Model Statistics:**\n\n* **Residual standard error:** 3.046 on 30 degrees of freedom, indicating moderate variability in the residuals.\n* **Multiple R-squared:** 0.7528, indicating a significant linear relationship between `mpg` and `wt`.\n* **Adjusted R-squared:** 0.7446, which is slightly lower than the multiple R-squared due to the presence of non-constant variance (heteroscedasticity).\n* **F-statistic:** 91.38 on 1 and 30 degrees of freedom, with a p-value: 1.294e-10.\n* The F-statistic indicates that the model is highly significant at any plausible level.\n\n**Conclusion:**\n\nThe linear regression model provides a good fit to the data, indicating that `mpg` is negatively related to `wt`. This makes sense physically, as heavier cars tend to have lower fuel efficiency. However, there are some residual issues (outliers and upward trend) that may require further investigation or modeling adjustments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#writing-knowledge-questions",
    "href": "ch3.html#writing-knowledge-questions",
    "title": "\n3  Text Generation\n",
    "section": "\n3.3 Writing Knowledge Questions",
    "text": "3.3 Writing Knowledge Questions\n\n3.3.1 English\nWrite the LLM role and user query,\n\nsystem_instruction = \"\nYou are an expert in public health. Currently you are tasked to write questions \naccording to given query. Give answers to each question. Take into account \ntarget population of the questions.\n\"\nuser_query = \"\nWrite 10 knoweldge questions about malaria for general public with low education level.\n\"\n\nCombine these in a tibble,\n\nq_text = tribble(\n  ~role,    ~content,\n  \"system\", system_instruction,\n  \"user\",   user_query\n)\nq_text\n\n\n  \n\n\n\nAsk the LLM. Look at Question 2, what’s wrong? That’s why we must evaluate texts given by LLM carefully.\n\nquery(q_text, llama, output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\nHere are 10 knowledge questions about malaria suitable for a general public with low education level:\n\n1. What is malaria?\na) A type of food poisoning\nb) A disease that affects the eyes\nc) A sickness caused by a tiny parasite that is spread by mosquitoes\nd) A type of skin cancer\n\nAnswer: c) A sickness caused by a tiny parasite that is spread by mosquitoes\n\n2. Which mosquito is most likely to spread malaria?\na) Housefly\nb) Mosquito\nc) Sand fly\nd) Tsetse fly\n\nAnswer: b) Mosquito\n\n3. What are the symptoms of malaria?\na) Fever, chills, and headache\nb) Diarrhea, vomiting, and stomach pain\nc) Cough, runny nose, and sore throat\nd) Muscle weakness, joint pain, and fatigue\n\nAnswer: a) Fever, chills, and headache\n\n4. How is malaria typically spread?\na) Through touch or contact with an infected person\nb) Through eating contaminated food or water\nc) Through the bite of an infected mosquito\nd) Through breathing in infected air\n\nAnswer: c) Through the bite of an infected mosquito\n\n5. Can anyone get malaria?\na) No, it only affects children and pregnant women\nb) Yes, but mostly people who live in tropical areas\nc) Only people with weak immune systems\nd) Everyone can get malaria if bitten by an infected mosquito\n\nAnswer: d) Everyone can get malaria if bitten by an infected mosquito\n\n6. How is malaria diagnosed?\na) By looking at the patient's symptoms and medical history\nb) By performing a blood test to check for the parasite\nc) By examining the patient's stool or urine\nd) By doing a physical examination of the patient's body\n\nAnswer: b) By performing a blood test to check for the parasite\n\n7. What is the treatment for malaria?\na) Taking antibiotics\nb) Drinking plenty of water and resting\nc) Taking antimalarial medication such as chloroquine or artemisinin\nd) Getting a vaccine to prevent it from happening again\n\nAnswer: c) Taking antimalarial medication such as chloroquine or artemisinin\n\n8. Can malaria be prevented?\na) Yes, by wearing long-sleeved clothes and applying insecticide\nb) No, it's only a matter of luck if you get bitten by an infected mosquito\nc) Only pregnant women can prevent malaria with special medication\nd) By eating more fruits and vegetables\n\nAnswer: a) Yes, by wearing long-sleeved clothes, applying insecticide, and taking preventive measures such as using bed nets\n\n9. How can I protect myself from mosquitoes that may carry malaria?\na) Wear dark colors to blend in with the surroundings\nb) Avoid going outside during peak mosquito hours\nc) Use a net around your bed to keep mosquitoes away\nd) Use insecticide on your skin and clothes\n\nAnswer: c) Use a net around your bed to keep mosquitoes away and b) Avoid going outside during peak mosquito hours\n\n10. Who is most at risk of getting malaria?\na) People who live in urban areas\nb) Children under 5 years old\nc) Pregnant women\nd) All of the above\n\nAnswer: d) All of the above, especially those living in tropical and subtropical regions\n\n\n\n3.3.2 Malay\nWrite the LLM role and user query,\n\nsystem_instruction = \"\nAnda pakar dalam kesihatan awam. Anda ditugaskan untuk menulis soalan. \nBeri jawapan kepada setiap soalan yang anda tulis. Ambil kira populasi sasaran soalan.\n\"\nuser_query = \"\nTulis 10 soalan pengetahuan tentang demam denggi untuk orang awam.\n\"\n\nAt the moment, for this Malaysian LLM, you need to simplify the instructions as it is still in its early stages. Note that the model understands English because it is based on LLama 3 (note the model’s name). You may also explore the MaLLaM model (https://mesolitica.com/mallam).\nCombine these in a tibble,\n\nq_text = tribble(\n  ~role,    ~content,\n  \"system\", system_instruction,\n  \"user\",   user_query\n)\nq_text\n\n\n  \n\n\n\nAsk the Malaysian LLM hf.co/RichardErkhov/mesolitica_-_malaysian-llama-3-8b-instruct-16k-gguf:Q4_K_M,\n\nquery(q_text, malay, output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\nSudah tentu, saya boleh membantu. Berikut adalah beberapa soalan dan jawapan mengenai demam denggi:\n\n1. Apakah demam denggi?\nDemam denggi ialah penyakit yang disebabkan oleh jangkitan virus Denggi.\n\n2. Bagaimana demam denggi merebak?\nDemam denggi merebak melalui gigitan nyamuk Aedes aegypti yang membawa virus Denggi.\n\n3. Apakah gejala demam denggi?\nGejala awal demam denggi termasuk demam tinggi, sakit kepala, ruam kulit dan loya-loya. Gejala lain termasuk keletihan, muntah-muntah, dan gangguan pernafasan.\n\n4. Apakah faktor risiko jangkitan demam denggi?\nFaktor risiko jangkitan demam denggi ialah tinggal di kawasan yang terdapat banyak nyamuk Aedes aegypti, seperti kawasan bandar atau pinggir bandar.\n\n5. Bagaimana cara mencegah penularan demam denggi?\nCara mencegah penularan demam denggi termasuk mengelakkan gigitan nyamuk dengan memakai pakaian yang menutupi kulit, menggunakan repelan serangga, dan membersihkan tempat pembiakan nyamuk seperti kolam atau bekas air yang bertakung.\n\n6. Bagaimana cara mengenal pasti demam denggi?\nDemam tinggi adalah salah satu gejala utama demam denggi. Gejala lain termasuk ruam kulit, sakit kepala, dan gangguan pernafasan.\n\n7. Adakah rawatan untuk demam denggi wujud?\nYa, terdapat beberapa jenis ubat yang boleh digunakan untuk merawat demam denggi, seperti parasetamol untuk mengurangkan demam dan ubat anti-radang untuk meredakan gejala.\n\n8. Apakah risiko jangkitan semula demam denggi?\nJika anda pernah menghidap demam denggi sebelum ini, risiko jangkitan semula boleh meningkat jika tidak berhati-hati dalam menjaga kebersihan dan mencegah gigitan nyamuk.\n\n9. Bagaimana cara menguruskan gejala demam denggi?\nAdalah penting untuk memantau gejala anda dan segera mencari bantuan perubatan jika mengalami sakit kepala yang teruk, gangguan pernafasan, atau tanda-tanda komplikasi lain.\n\n10. Adakah vaksin untuk demam denggi?\nPada masa ini, tiada vaksin berkesan tersedia untuk melindungi daripada jangkitan demam denggi. Walau bagaimanapun, penyelidikan dan pembangunan terus dijalankan untuk mencari penyelesaian yang lebih baik dalam mencegah dan mengawal demam denggi.\n\nSaya harap jawapan ini membantu! Jika anda memerlukan maklumat lanjut, sila beritahu saya.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#r-coding",
    "href": "ch3.html#r-coding",
    "title": "\n3  Text Generation\n",
    "section": "\n3.4 R Coding",
    "text": "3.4 R Coding\nSpecify your coding problem,\n\nq_text = \"\nShow how to perform a logistic regression analysis in R programming language.\n\"\n\nAsk for help. Here we try a larger model, qwen2.5-coder, a 7B model.\n\ncoder = \"qwen2.5-coder:7b\"\nquery(q_text, coder, output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\nTo perform a logistic regression analysis in R, you can use the glm() function which stands for Generalized Linear Model.\n\nFirstly, make sure that you have a dataset. If not, you could use the built-in 'mtcars' or 'iris' datasets or create your own dataset.\n\nLet's say our data is stored in 'data', and we want to perform logistic regression on a binary outcome variable called 'outcome' based on several predictor variables including 'predictor1', 'predictor2', etc. \n\nThe glm() function can be used as follows:\n\n```{r}\n# Load the necessary library if not already loaded\nlibrary(stats)\n\n# Define the formula for your model. The formula here is logistic regression with outcome dependent on predictor1 and predictor2.\nmodel_formula &lt;- outcome ~ predictor1 + predictor2\n\n# Use glm to fit a logistic regression model using the defined formula and data\nlogistic_model &lt;- glm(model_formula, data = data, family=binomial())\n\n# Summary of the model output\nsummary(logistic_model)\n```\n\nThe summary will provide the coefficients for your predictors which indicate their influence on the likelihood of the outcome.\n\nRemember to replace 'outcome', 'predictor1', and 'predictor2' with your actual variable names.\n\nAlso, always remember that when it comes to logistic regression, it is crucial to check the assumptions of this model. This includes checking if the relationship between the independent variables and the logit of the dependent variable is linear, whether all the observations have equal weights (no overfitting or underfitting), and finally checking for the independence of errors (there should be no correlation among residuals). \n\nIn case the assumptions are not met, you might need to consider using a different model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#reference-formatter",
    "href": "ch3.html#reference-formatter",
    "title": "\n3  Text Generation\n",
    "section": "\n3.5 Reference Formatter",
    "text": "3.5 Reference Formatter\nReferences can be converted from one referencing format to another. Here, we have several references to be formatted (taken from https://doi.org/10.1016/j.gore.2022.101024),\n\nrefs = \"\nBekkers, S., Bot, A.G., Makarawung, D., et al., 2014. The National Hospital Discharge\nSurvey and Nationwide Inpatient Sample: the databases used affect results in THA\nresearch. Clin. Orthop. Relat. Res. 472, 3441–3449.\n\nChubak, J., Ziebell, R., Greenlee, R.T., Honda, S., et al., 2016. The Cancer Research\nNetwork: a platform for epidemiologic and health services research on cancer\nprevention, care, and outcomes in large, stable populations. Cancer Causes Control.\n27 (11), 1315–1323.\n\nDreyer, N.A., Tunis, S.R., Berger, M., Ollendorf, D., Mattox, P., Gliklich, R., 2010. Why\nobservational studies should be among the tools used in comparative effectiveness\nresearch. Health Aff. (Millwood). 29 (10), 1818–1825.\n\nHusereau, D., Drummond, M., Augustovski, F., et al., 2022. Consolidated health\neconomic evaluation reporting standards 2022 (CHEERS 2022) statement: updated\nreporting guidance for health economic evaluations. Int. J. Technol. Assess. Health\nCare. 38 (1).\n\"\n\nPlace everything together nicely in a tibble,\n\nq_text = tribble(\n  ~role,    ~content,\n  \"system\", \"Convert the given references in APA 7 format. Do not comment or elaborate.\",\n  \"user\",   refs\n)\nq_text\n\n\n  \n\n\n\nAsk it to do the job,\n\nquery(q_text, llama, output = \"text\", screen = FALSE,\n      model_params = list(num_ctx = 2000)) |&gt; cat()\n\nBekkers, S., Bot, A. G., Makarawung, D., et al., (2023, December). The National Hospital Discharge Survey and Nationwide Inpatient Sample: the databases used affect results in Total Hip Arthroplasty research. Journal of Orthopaedic Research, 472(10), 3441–3449.\n\nChubak, J., Ziebell, R., Greenlee, R. T., Honda, S., et al., (2022). The Cancer Research Network: a platform for epidemiologic and health services research on cancer prevention, care, and outcomes in large, stable populations. Cancer Causes Control, 27(10), 1315–1323.\n\nDreyer, N. A., Tunis, S. R., Berger, M., Ollendorf, D., Mattox, P., Gliklich, R., (2020). Why observational studies should be among the tools used in comparative effectiveness research. Health Affairs, 39(10), 1818–1825.\n\nHusereau, D., Drummond, M., Augustovski, F., et al., (2023). Consolidated health economic evaluation reporting standards 2022 (CHEERS 2022) statement: updated reporting guidance for health economic evaluations. International Journal of Technology Assessment in Health Care, 39(1), e123–e134.\n\n\nHere we increase the context window to 2000. Context window/size is number of tokens that can be LLM can receive/produce as input/output. It is around 3/2 times words in a given text. Please ask your LLM companion for more information about it :-)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#summarizing-abstracts-from-pubmed",
    "href": "ch3.html#summarizing-abstracts-from-pubmed",
    "title": "\n3  Text Generation\n",
    "section": "\n3.6 Summarizing Abstracts From PubMed",
    "text": "3.6 Summarizing Abstracts From PubMed\nGet abstract from PubMed on Partial verification bias from year 2020 until 2025. This requires pubmedR (Aria, 2020) and bibliometrix (Aria & Cuccurullo, 2017, 2025) packages.\n\nlibrary(pubmedR)\nlibrary(bibliometrix)\nlibrary(stringr)\napi_key = NULL\nquery_pvb = \"partial verification bias*[Title/Abstract] AND english[LA] AND Journal Article[PT] AND 2020:2025[DP]\"\nres = pmQueryTotalCount(query = query_pvb, api_key = api_key)\nD = pmApiRequest(query = query_pvb, limit = res$total_count, api_key = api_key)\n\nDocuments  14  of  14 \n\nM = convert2df(D, dbsource = \"pubmed\", format = \"api\")\n\n\nConverting your pubmed collection into a bibliographic dataframe\n\n=============================================================================\nDone!\n\n\nSave relevant abstracts in text_abs. We may flatten the abstracts into a single text string, text_abs_flat,\n\nkey = grep(\"PARTIAL VERIFICATION BIAS\", M$TI)  # Select abstract with \"partial verification bias\"\ntext_abs = M$AB[key]\ntext_abs_flat = str_flatten(M$AB[key])\n\nEstimate the context size required by the LLM model,\n\nctx = str_length(text_abs_flat) * 3 / 2\nctx = round(ctx, -3)  # with large ctx, be careful with VRAM use\nctx\n\n[1] 5000\n\n\nSetup the query in a tibble,\n\nq_text = tribble(\n  ~role, ~content,\n  \"system\", \"Summarize the content of the given text.\",\n  \"user\", text_abs_flat,\n)\n\nand get the result,\n\nquery(q_text, llama, output = \"text\", screen = FALSE, \n      model_params = list(num_ctx = ctx)) |&gt; cat()\n\nThe text discusses the importance of evaluating new diagnostic tests in medical care, particularly when it comes to accuracy measures such as sensitivity (SN) and specificity (SP). However, these measures can be biased due to a phenomenon called partial verification bias (PVB), where only certain patients are verified.\n\nTo address this issue, the article introduces Inverse Probability Bootstrapping (IPB) sampling as a method to correct for PVB. IPB is adapted for use in correcting PVB and tested on simulated and clinical data sets. The results show that IPB is accurate for estimating SN and SP, but less precise than existing methods.\n\nThe article also discusses alternative approaches to avoiding PVB, such as applying the reference standard to all individuals who are positive on the index test and a random sample of those who are negative. It highlights the need for further research to reduce the standard error (SE) of IPB.\n\nOverall, the text emphasizes the importance of thoroughly evaluating new diagnostic tests in medical care, particularly when it comes to accuracy measures, and discusses various methods available to correct for PVB.\n\n\nYou will notice that it refers to “an article” because we combined the abstracts in a single text.\nAnd we may try using llm_vec_summarize() from mall1 package (Ruiz, 2024) to summarize each abstract in 20 words or less,\n\nlibrary(mall)\nllm_use(\"ollama\", llama, .silent = TRUE)\nllm_vec_summarize(text_abs, max_words = \"20\") -&gt; text_abs_summ\ntext_abs_summ\n\n[1] \"ipb is accurate for sn and sp estimation with low bias but less precise than existing methods with higher se.\"                    \n[2] \"partial verification bias occurs when a positive index test makes application of the reference standard more likely.\"             \n[3] \"new diagnostic tests undergo evaluation comparing to gold standard tests with accuracy measures like sensitivity and specificity.\"\n\ntext_abs_summ |&gt; str_split(\" \") |&gt; sapply(length)\n\n[1] 20 17 17",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#extracting-information",
    "href": "ch3.html#extracting-information",
    "title": "\n3  Text Generation\n",
    "section": "\n3.7 Extracting Information",
    "text": "3.7 Extracting Information\nWe can extract relevant information from a given text. As an example, I provide it with an announcement for a recent webinar,\n\nwebinar_text = \"\nTopic: Easy Data Exploration and Visualization using GWalkR Package in R\nDate: 18 January 2025\nTime: 9.00pm \nPlatform: Webex\n\nOrganized by the Malaysian Disease Modelling Expert Group (MDMEG)\n\"\nllm_use(\"ollama\", \"gemma2:2b\", .silent = TRUE, seed = 111)  # using smaller model; gemma2:2b\nllm_vec_extract(webinar_text, c(\"title\", \"date\", \"time\", \"place\", \"organizer\")) |&gt; cat()\n\ntitle|date|time|place|organizer\neasy data exploration and visualization using gwalkr package in r|january 18, 2025|9:00pm|webex|malasian disease modelling expert group (mdmeg) \n\n\nNotice that, although we uses the words “title” and “place” instead of “topic” and “platform”, it could still understand these terms. I also used seed = 111, because I was satisfied with the given response using this seed number. You may try with other seed numbers and see the changes in the responses.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#translation",
    "href": "ch3.html#translation",
    "title": "\n3  Text Generation\n",
    "section": "\n3.8 Translation",
    "text": "3.8 Translation\nWe can try the llm_vec_translate() (or llm_translate() if your input is a data frame or tibble) from mall. Here, we translate the first abstract from English to Malay,\n\ntext_abs[1] # English text\n\n[1] \"IN MEDICAL CARE, IT IS IMPORTANT TO EVALUATE ANY NEW DIAGNOSTIC TEST IN THE FORM OF DIAGNOSTIC ACCURACY STUDIES. THESE NEW TESTS ARE COMPARED TO GOLD STANDARD TESTS, WHERE THE PERFORMANCE OF BINARY DIAGNOSTIC TESTS IS USUALLY MEASURED BY SENSITIVITY (SN) AND SPECIFICITY (SP). HOWEVER, THESE ACCURACY MEASURES ARE OFTEN BIASED OWING TO SELECTIVE VERIFICATION OF THE PATIENTS, KNOWN AS PARTIAL VERIFICATION BIAS (PVB). INVERSE PROBABILITY BOOTSTRAP (IPB) SAMPLING IS A GENERAL METHOD TO CORRECT SAMPLING BIAS IN MODEL-BASED ANALYSIS AND PRODUCES DEBIASED DATA FOR ANALYSIS. HOWEVER, ITS UTILITY IN PVB CORRECTION HAS NOT BEEN INVESTIGATED BEFORE. THE OBJECTIVE OF THIS STUDY WAS TO INVESTIGATE IPB IN THE CONTEXT OF PVB CORRECTION UNDER THE MISSING-AT-RANDOM ASSUMPTION FOR BINARY DIAGNOSTIC TESTS. IPB WAS ADAPTED FOR PVB CORRECTION, AND TESTED AND COMPARED WITH EXISTING METHODS USING SIMULATED AND CLINICAL DATA SETS. THE RESULTS INDICATED THAT IPB IS ACCURATE FOR SN AND SP ESTIMATION AS IT SHOWED LOW BIAS. HOWEVER, IPB WAS LESS PRECISE THAN EXISTING METHODS AS INDICATED BY THE HIGHER STANDARD ERROR (SE). DESPITE THIS ISSUE, IT IS RECOMMENDED TO USE IPB WHEN SUBSEQUENT ANALYSIS WITH FULL DATA ANALYTIC METHODS IS EXPECTED. FURTHER STUDIES MUST BE CONDUCTED TO REDUCE THE SE.\"\n\nllm_use(\"ollama\", malay, .silent = TRUE, seed = 123)\nllm_vec_translate(text_abs[1], language = \"Malay\")  # Malay text\n\n[1] \"Dalam penjagaan kesihatan, adalah penting untuk menilai sebarang ujian diagnostik baharu menggunakan kajian ketepatan diagnostik. Ujian ini dibandingkan dengan ujian standard emas, di mana prestasi ujian diagnostik binari biasanya diukur oleh kepekaan (SN) dan kekhususan (SP). Walau bagaimanapun, ukuran ketepatan ini sering berat sebelah kerana pengesahan terpilih pesakit, yang dikenali sebagai bias pengesahan separa (PVB). Pemampatan butstrap kebarangkalian songsang (IPB) ialah kaedah umum untuk membetulkan berat sebelah pensampelan dalam analisis berasaskan model dan menghasilkan data berat sebelah untuk dianalisis. Walau bagaimanapun, penggunaannya dalam pembetulan PVB belum disiasat sebelum ini. Objektif kajian adalah untuk menyiasat IPB dalam konteks pembetulan PVB di bawah andaian hilang rawak untuk ujian diagnostik binari. IPB telah disesuaikan untuk pembetulan PVB dan diuji serta dibandingkan dengan kaedah sedia ada menggunakan set data simulasi dan klinikal. Keputusan menunjukkan bahawa IPB adalah tepat untuk anggaran SN dan SP kerana ia mempamerkan berat sebelah yang rendah. Walau bagaimanapun, IPB kurang tepat daripada kaedah sedia ada seperti yang ditunjukkan oleh ralat piawai (SE) yang lebih tinggi. Walaupun isu ini, disyorkan untuk menggunakan IPB apabila analisis susulan dengan kaedah analitik data penuh dijangka. Kajian lanjut perlu dilakukan untuk mengurangkan SE.\"\n\n\nYou can try to replicate this using rollama’s query() function.2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#querying-multiple-local-llms",
    "href": "ch3.html#querying-multiple-local-llms",
    "title": "\n3  Text Generation\n",
    "section": "\n3.9 Querying Multiple Local LLMs",
    "text": "3.9 Querying Multiple Local LLMs\nYou can easily send a query to multiple LLMs at once in rollama.\n\nq_text = \"Provide a brief description of hypertension.\"\nq_response = query(q_text, c(llama, \"gemma2:2b\", \"phi3:3.8b\", \"mistral:7b\"),\n                   output = \"data.frame\", screen = FALSE,\n                   model_params = list(seed = 123))\nq_response\n\n\n  \n\n\n\nHere, we saved the output as data.frame, which can be easily accessed later. We also included model_params = list(seed = 123) to get consistently generated texts for the query. You can set the seed for the whole R session by using options(rollama_seed = 123) function, which can be placed right after library(rollama).\nThe responses for the query are displayed in Table tbl-multi-llm.\n\nknitr::kable(q_response, format = \"html\",\n             caption = 'Query - \"Provide a brief description of hypertension.\"',\n             col.names = c(\"Model\", \"Role\", \"Response\"))\n\n\n\n\nTable 3.1: Query - \"Provide a brief description of hypertension.\"\n\n\n\n\nModel\nRole\nResponse\n\n\n\nllama3.2:3b\nassistant\nHypertension, also known as high blood pressure, is a medical condition in which the force of blood against the artery walls is consistently too high. It can lead to damage and strain on the heart, kidneys, brain, and other vital organs.\\n\\nNormally, when the heart beats, it pumps blood into the arteries, causing them to constrict (or narrow) slightly. As the heart relaxes, the arteries dilate (or widen), allowing fresh blood to flow in. In people with hypertension, this process is disrupted, leading to:\\n\\n1. Arteries that remain permanently narrowed or constricted.\\n2. Blood vessels that are stiff and less able to stretch.\\n\\nHigh blood pressure increases the risk of various health problems, including heart attack, stroke, kidney disease, and vision loss. It can also lead to organ damage over time if left untreated.\\n\\nHypertension is often asymptomatic, making it crucial for people with high blood pressure to monitor their condition regularly through regular check-ups and at-home blood pressure monitoring.\n\n\ngemma2:2b\nassistant\nHypertension, also known as high blood pressure, is a condition where the force of your blood against your artery walls is consistently too high. \\n\\n**Here's a simplified breakdown:**\\n\\n* **High Blood Pressure (BP):** Imagine a hose with water flowing through it. In hypertensive people, the water pressure in the hose (blood) is too high, causing strain on the walls and potentially leading to problems. \\n* **Causes:** Many factors contribute to hypertension, including genetics, lifestyle choices (diet, exercise), stress, and underlying medical conditions like diabetes or kidney disease.\\n* **Consequences:** Uncontrolled hypertension damages blood vessels over time, increasing risk of heart attack, stroke, vision loss, and other serious health problems. \\n\\n**It's important to note that:** Hypertension can be silent in early stages, so regular checkups are crucial. \\n\\n\\nIf you have concerns about your blood pressure, please consult a healthcare professional for accurate diagnosis and guidance.\\n\n\n\nphi3:3.8b\nassistant\nHypertension, also known as high blood pressure, is a common condition in which the force against your artery walls is consistently too high. This can lead to health problems such as heart disease and stroke if not managed properly. There are two types: primary (essential) hypertension, often caused by factors like genetics or lifestyle; and secondary hypertension due to an underlying condition that affects the kidneys, arteries, heart, or endocrine system. It is typically asymptomatic but can be detected through routine screening.\n\n\nmistral:7b\nassistant\nHypertension, also known as high blood pressure, is a long-term medical condition in which the force of blood against the walls of arteries is consistently too high. This pressure causes the heart to work harder than normal to pump blood throughout the body, potentially leading to health problems such as heart disease, stroke, kidney failure, and other complications if left untreated. Hypertension is typically categorized as either primary (essential) hypertension or secondary hypertension, with primary being more common and having no identifiable underlying cause, while secondary hypertension is caused by an underlying condition such as kidney problems, tumors, or certain medications.\n\n\n\n\n\n\n\n\nand you can get well-formatted text if it is processed as Markdown,\n\ncat(q_response$response[2])\n\n\nHypertension, also known as high blood pressure, is a condition where the force of your blood against your artery walls is consistently too high.\nHere’s a simplified breakdown:\n\n\nHigh Blood Pressure (BP): Imagine a hose with water flowing through it. In hypertensive people, the water pressure in the hose (blood) is too high, causing strain on the walls and potentially leading to problems.\n\nCauses: Many factors contribute to hypertension, including genetics, lifestyle choices (diet, exercise), stress, and underlying medical conditions like diabetes or kidney disease.\n\nConsequences: Uncontrolled hypertension damages blood vessels over time, increasing risk of heart attack, stroke, vision loss, and other serious health problems.\n\nIt’s important to note that: Hypertension can be silent in early stages, so regular checkups are crucial.\nIf you have concerns about your blood pressure, please consult a healthcare professional for accurate diagnosis and guidance.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#deep-dive-how-it-works",
    "href": "ch3.html#deep-dive-how-it-works",
    "title": "\n3  Text Generation\n",
    "section": "\n3.10 Deep-dive: How It Works",
    "text": "3.10 Deep-dive: How It Works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#references",
    "href": "ch3.html#references",
    "title": "\n3  Text Generation\n",
    "section": "References",
    "text": "References\n\n\n\n\nAria, M. (2020). pubmedR: Gathering metadata about publications, grants, clinical trials from PubMed database. Retrieved from https://github.com/massimoaria/pubmedR\n\n\nAria, M., & Cuccurullo, C. (2017). Bibliometrix: An r-tool for comprehensive science mapping analysis. Journal of Informetrics. https://doi.org/10.1016/j.joi.2017.08.007\n\n\nAria, M., & Cuccurullo, C. (2025). Bibliometrix: Comprehensive science mapping analysis. Retrieved from https://www.bibliometrix.org\n\n\nGruber, J. B., & Weber, M. (2025). Rollama: Communicate with ollama to run large language models locally. Retrieved from https://jbgruber.github.io/rollama/\n\n\nLin, H., & Safi, T. (2025). Ollamar: Ollama language models. Retrieved from https://hauselin.github.io/ollama-r/\n\n\nRuiz, E. (2024). Mall: Run multiple large language model predictions against a table, or vectors. Retrieved from https://mlverse.github.io/mall/",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch3.html#footnotes",
    "href": "ch3.html#footnotes",
    "title": "\n3  Text Generation\n",
    "section": "",
    "text": "mall relies on ollamar (Lin & Safi, 2025). You can see detailed ollamar code by including preview = TRUE option to mall’s functions.↩︎\nHint: You can see how llm_translate() does it by including preview = TRUE option.↩︎",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Text Generation</span>"
    ]
  },
  {
    "objectID": "ch4.html",
    "href": "ch4.html",
    "title": "\n4  Vision\n",
    "section": "",
    "text": "4.1 Preliminaries\nAnother prevalent use of LLMs is in the form of vision language models (VLMs). In the previous chapter, tasks involved generating text from text input alone; now these tasks include generating text based on image and text instructions. For example, we might want a VLM to write a short comment about a graph or even perform optical character recognition (OCR) tasks. Unlike typical LLMs that do not have the capability to process images directly, VLMs are enabled with the ability to read images, typically in base64 format, which is commonly used to embed images in HTML files. Base64-encoded images are essentially strings of text, explaining why vision language models can effectively handle image data.\nIn this chapter, we will explore how to perform several image-and-text to text generation tasks in R using the rollama package for a variety of relevant use cases in academic and medical research. Again, for the generated text, you will actually get different results from mine due to the random nature of text generation process.\nLoad these packages,\nlibrary(rollama)\nlibrary(purrr)\nlibrary(tibble)\nlibrary(base64enc) # to convert image to base64 format\nMake sure that your Ollama is running,\nping_ollama()  # ensure it's running\n\n▶ Ollama (v0.5.1) is running at &lt;http://localhost:11434&gt;!\n\n# list_models()$name #&gt; run this to view available models in your system\nGive names to our buddies,\nmoondream = \"moondream\" # small vision model\nminicpm = \"minicpm-v\" # uses Qwen2:7b\nllava = \"llava-llama3\" # llava + llama3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#interpreting-an-image",
    "href": "ch4.html#interpreting-an-image",
    "title": "\n4  Vision\n",
    "section": "\n4.2 Interpreting an Image",
    "text": "4.2 Interpreting an Image\nSuppose we want a VLM to describe the image of the NVIDIA GPU in Chapter 2, which I placed at the path “img/nvidia.jpeg” in my local PC.\n\n\nThe NVIDIA RTX4090. Source: https://www.nvidia.com/en-us/geforce/graphics-cards/\n\nWe can use the same query() function, with the addition of images = option,\n\nquery(\"Describe the image:\", moondream, images = \"img/nvidia.jpeg\",\n      output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\nThe image features a computer graphics rendering of a computer chip, specifically a graphics processing unit (GPU). The GPU is positioned in the center of the frame and appears to be facing towards the right side. It has a black and silver color scheme with a fan located on top of it. The background of the image consists of green lines that extend from both sides of the frame, creating an abstract pattern.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#interpreting-multiple-images",
    "href": "ch4.html#interpreting-multiple-images",
    "title": "\n4  Vision\n",
    "section": "\n4.3 Interpreting Multiple Images",
    "text": "4.3 Interpreting Multiple Images\nThe images = option can support multiple images. Now, we try more than one image,\n\nquery(\"Describe the images:\",\n      llava, # model dependent, llava can interpret more than 1 image\n      images = c(\"img/gpt_down.png\", \"img/gpt_limit.png\"))\n\n\n\n\n── Answer from llava-llama3 ─────────────────────────────────────────────────\n\n\nThe image captures a screenshot of a message from the Chatbot on a messaging\nplatform. The Chatbot, named \"ChatGPT\", is displayed with a blue header and\nfooter against a white background. The header prominently displays the words\n\"New chat\" in black text, indicating that a new chat has been initiated.\n\n\n\n\n\nJust below the header, there's a brief introduction of the Chatbot in black\ntext: \"You're hit the Free plan limit for CGPT-0\". This suggests that the\nuser has exceeded the free plan limit for CGPT-0 and is now required to\nupgrade to a paid plan.\n\n\n\n\n\nThe footer of the message contains two options for the user: \"Get Plus\" and\n\"Message ChatGPT\". The \"Get Plus\" option is presumably for upgrading to a\npaid plan, while the \"Message ChatGPT\" option allows the user to initiate a\nnew chat.\n\n\n\n\n\nIn the bottom right corner of the screenshot, there's a small icon with a\nblue circle and white plus sign inside it, possibly indicating that the user\nhas unread messages in their inbox or has a pending request. The overall\ndesign of the message suggests a simple yet effective way of communicating\nwith users about plan limits and upgrading options on the messaging\nplatform.\n\n\nNot all VLM supports multiple images. You can try with minicpm-v, and you will be greeted with an error.\n\nquery(\"Describe the images:\",\n      minicpm, # model dependent, llava can interpret more than 1 image\n      images = c(\"img/gpt_down.png\", \"img/gpt_limit.png\"))\n\nError in `map()`:\nℹ In index: 1.\nCaused by error in `strsplit()`:\n! non-character argument",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#interpreting-a-plot",
    "href": "ch4.html#interpreting-a-plot",
    "title": "\n4  Vision\n",
    "section": "\n4.4 Interpreting a Plot",
    "text": "4.4 Interpreting a Plot\nPlot MPG from mtcars dataset,\n\nhist(mtcars$mpg, main = \"Histogram of MPG\", xlab = \"MPG\")\n\n\n\n\n\n\n\nBecause query() requires image input for images = option, we save the histogram as “img.png” first,\n\nimage_name = \"img.png\"\npng(image_name)\nhist(mtcars$mpg, main = \"Histogram of MPG\", xlab = \"MPG\")\ndev.off()\n\npng \n  2 \n\n\nNow, we can send the query a VLM model to describe the image,\n\nquery(\"Describe the image:\", llava, images = image_name,\n      output = \"text\", screen = FALSE) |&gt; cat()\n\n\n\nThe provided histogram displays a distribution of miles per gallon (MPG) for vehicles. The x-axis represents MPG values ranging from 10 to 35, while the y-axis shows the frequency of each MPG value encountered in a dataset or study.\n\nKey observations include:\n- A significant number of vehicle models achieve an MPGe of approximately 26.\n- Another notable peak is observed at around 19.5 MPGe with frequent occurrences.\n- The highest frequencies are clustered between 10 and 30 MPG, particularly noticeable for the range from 15 to 27 MPG.\n\nThis distribution helps in understanding typical fuel efficiency across vehicle models studied or a specific dataset analyzed.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#r-coding-from-image",
    "href": "ch4.html#r-coding-from-image",
    "title": "\n4  Vision\n",
    "section": "\n4.5 R Coding from Image",
    "text": "4.5 R Coding from Image\nMaybe you came across a nice looking plot, and you wonder how to come up with a similar plot in R. Let’s try with this plot,\n\n\nA decent looking histogram. Source: https://wnarifin.github.io/2024-r-conf-my-pres-R/#39\n\nWe try utilizing minicpm-v for this task, which relies on Qwen2:7b. I found the LLM good for R coding as compared to llava-llama3,\n\nquery(\"Write R code to plot the image:\", minicpm, images = \"img/histogram.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 111)) |&gt; cat()\n\n\n\nTo create a histogram similar to the one shown in the provided figures, you can use the `ggplot2` package in R. Here's an example of how you might write this:\n\n```r\n# Required packages\nlibrary(ggplot2)\n\n# Assume we have data stored as vectors for simplicity.\nweights &lt;- c(105, 98, 73, 64, 51) # Example weights\n\n# Plotting the histogram using ggplot2\nggplot(data.frame(A = rep(weights, each = length(weights))), aes(x = A)) +\n  geom_histogram(binwidth = 1,\n                 fill = \"lightyellow\",\n                 color = \"black\",\n                 binlimit = c(0, 100))\n```\n\nThis code snippet uses `ggplot2` to create a histogram where the weights are represented in separate groups. You'll need to replace `weights` with your actual dataset if you have more data points.\n\nRemember that this example is just one approach and the exact details of how you use it will depend on your specific requirements!\n\n\nNow we copy and paste, and execute the generated code,\n\n# Required packages\nlibrary(ggplot2)\n\n# Assume we have data stored as vectors for simplicity.\nweights &lt;- c(105, 98, 73, 64, 51) # Example weights\n\n# Plotting the histogram using ggplot2\nggplot(data.frame(A = rep(weights, each = length(weights))), aes(x = A)) +\n  geom_histogram(binwidth = 1,\n                 fill = \"lightyellow\",\n                 color = \"black\",\n                 binlimit = c(0, 100))\n\nWarning in geom_histogram(binwidth = 1, fill = \"lightyellow\", color = \"black\",\n: Ignoring unknown parameters: `binlimit`\n\n\n\n\n\n\n\n\nWe noted that the plot we obtained does not resemble the provided image, except for the bar colour and the x-axis label. We also encountered an error from an unknown parameter binlimit. At times, our buddies can actually make-up non-existent options. But at least, it suggested using ggplot2 and fill = \"lightyellow\" among others.\nLet’s try with a larger LLM, llama3.2-vision, an 11b model.\n\nquery(\"Write R code to plot the image. Only return the raw code. Do not comment.\", \"llama3.2-vision\", images = \"img/histogram.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 123)\n      ) -&gt; tempa\ncat(tempa)\n\n```R\nlibrary(ggplot2)\n\ndata &lt;- data.frame(Weight = c(0, 20, 40, 60, 100), Frequency = c(0, 30, 120, 210, 0))\n\nggplot(data, aes(x = Weight, y = Frequency)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Histogram of Weight (kg)\", x = \"Weight (kg)\", y = \"Frequency\")\n```\n\n\nWe can save the generated code in a file namely tmp.R, then source() it,\n\ntempb = gsub(\"```R\", \"\", tempa)\ntempb = gsub(\"```r\", \"\", tempb)\ntempb = gsub(\"```\", \"\", tempb)\ntempb |&gt; cat() |&gt; capture.output(file = \"tmp.R\")\nsource(\"tmp.R\", print.eval=TRUE)\n\n\n\n\n\n\n\nof which, the color does not look right, although it is quite an improvement over the previous LLM model.\nLet’s cheat a bit, I obtained the following R code from OpenAI’s GPT4o with a prompt “Write R code to come up with this image.” for the same image,\n\n# Sample Data\nset.seed(123)\nweights &lt;- rnorm(1000, mean = 50, sd = 15)\n\n# Histogram Plot\nhist(weights,\n     breaks = 10,\n     col = \"cornsilk\",\n     border = \"black\",\n     main = \"Histogram of Weight (kg)\",\n     xlab = \"Weight (kg)\",\n     ylab = \"Frequency\",\n     xlim = c(0, 100),\n     ylim = c(0, 250),\n     las = 1,\n     cex.main = 1.5,\n     cex.lab = 1.2,\n     cex.axis = 1)\n\n\n\n\n\n\n\nwhich looks closely similar to the provided plot, although it did not use ggplot21. This seems to be the limitation of the local LLMs that we have tried so far.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#r-coding-from-image-two-step-approach",
    "href": "ch4.html#r-coding-from-image-two-step-approach",
    "title": "\n4  Vision\n",
    "section": "\n4.6 R Coding from Image – Two-step Approach",
    "text": "4.6 R Coding from Image – Two-step Approach\nInstead of relying on only one VLM, what if we combine a VLM with another LLM that is good at coding?\nLet’s try our idea. We start by asking llama3.2-vision to describe the image in detail,\n\nquery(\"Provide a clear and detailed description of the image to accurately recreate it as a chart. Include the type of plot, data points, axis labels, scale, colours, and any other relevant details needed for precise reproduction.\"\n      , \"llama3.2-vision\", images = \"img/histogram.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 111)\n      ) -&gt; temp1a\ncat(temp1a)\n\n\n\nBased on the provided description, here is a structured drawing of the histogram:\n\n**Title:** Histogram of Weight (kg)\n\n**Type:** Vertical Bar Plot\n\n**X-axis:**\n\n* **Major Ticks:** 0, 25, 50, 75, 100\n* **Minor Ticks:** Not explicitly mentioned, but implied as increments of 5 or 10 between major ticks\n* **Label:** Weight (kg)\n\n**Y-axis:**\n\n* **Major Ticks:** 0, 100, 200\n* **Minor Ticks:** Not explicitly mentioned, but implied as increments of 20 between major ticks\n* **Label:** Frequency\n\n**Bars:**\n\n* **Color:** Khaki (a shade of brown)\n* **Bar 1:** Weight 25 kg, Height approximately 40 units\n* **Bar 2:** Weight 50 kg, Height approximately 240 units\n* **Bar 3:** Weight 75 kg, Height approximately 120 units\n* **Bar 4:** Weight 100 kg, Height approximately 0 units\n\n**Observations:**\n\n* The histogram shows a skewed distribution with most weights concentrated around 50 kg.\n* There are no weights below 25 kg or above 100 kg.\n* The frequency of weights decreases as the weight increases beyond 75 kg.\n\n\nThen we ask qwen2.5-coder:7b to generate the code for the image description,\n\nq_text = paste(\"Write R code to plot a chart based description below. Only return the raw code. Do not comment.\\n\\n\", temp1a)\nquery(q_text, \"qwen2.5-coder:7b\", output = \"text\", screen = FALSE,\n      model_params = list(seed = 111)) -&gt; temp1b\ncat(temp1b)\n\n\n\n```R\n# Sample data for the histogram\nweights &lt;- c(25, 50, 75, 100)\nfrequencies &lt;- c(40, 240, 120, 0)\n\n# Create the histogram\nbarplot(frequencies, \n        names.arg = weights, \n        xlab = \"Weight (kg)\", \n        ylab = \"Frequency\", \n        main = \"Histogram of Weight (kg)\", \n        col = \"khaki\", \n        ylim = c(0, 250))\n```\n\n\nWe can save the generated code in a file namely tmp1.R, then source() it,\n\ntemp1c = gsub(\"```R\", \"\", temp1b)\ntemp1c = gsub(\"```r\", \"\", temp1c)\ntemp1c = gsub(\"```\", \"\", temp1c)\ntemp1c |&gt; cat() |&gt; capture.output(file = \"tmp1.R\")\nsource(\"tmp1.R\", print.eval=TRUE)\n\n\n\n\n\n\n\nNow, plot looks better with this two-step approach. Because the code generation is not deterministic in nature, you can always regenerate the code by removing the seed number.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#vlms-as-ocr-tools",
    "href": "ch4.html#vlms-as-ocr-tools",
    "title": "\n4  Vision\n",
    "section": "\n4.7 VLMs as OCR Tools",
    "text": "4.7 VLMs as OCR Tools\nLet’s try out VLMs as OCR tools. Say we have a PDF file from https://wnarifin.github.io/misc/Attitude_Statistics%20v3%20Questions.pdf.\n\n\nPDF to be converted\n\nFirst, convert the PDF (the PDF only contains one page) into PNG,\n\nlibrary(magick)\nimage = image_read_pdf(\"https://wnarifin.github.io/misc/Attitude_Statistics%20v3%20Questions.pdf\")\nimage_write(image, path = \"img/q.png\")\n\nThen, we ask llama3.2-vision to read the image and return the text,\n\nquery(\"Read the image and return the exact text only. Do not describe.\", \n      \"llama3.2-vision\", images = \"img/q.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 999)) |&gt; cat()\n\n\n\nThe following is the exact text from the image:\n\n**The following questions are about attitude towards statistics. Each question is rated on a scale ranging from 1 (strongly disagree) to 5 (strongly agree). Please circle your answer based on the scale accordingly.**\n\n**Questions:**\n\n1. I love all analysis.\n2. I dream of normal distribution.\n3. I think of probability in life.\n4. Statistics lecture is interesting.\n5. Statistics is easy to understand.\n6. I like statistics.\n7. I like to deal with data.\n8. I think statistics lecture is important.\n9. Statistics is important in research.\n10. Statistics lecture is important.\n11. I like to do statistical analysis.\n12. I carry statistics books wherever I go.\n\n**Note:** The image appears to be a survey or questionnaire related to attitudes towards statistics, but the specific context and purpose are not clear without additional information.\n\n\nLet’s compare the output to OCR,\n\nlibrary(tesseract)\nimg = image_read(\"img/q.png\")  # from `magick`\ntext = ocr(img)\ncat(text)\n\nThe following questions are about attitude towards statistics. Each question is\nrated on scale ranging from 1 (strongly disagree) to 5 (strongly agree). Please\ncircle your answer based on scale accordingly.\nStrongly Disagree Neutral Agree Strongly\ndisagree agree\n1 Ilove all analysis. 1 2 3 4 5\n2  |dream of normal distribution. 1 2 3 4 5\n3 | think of probability in life. 1 2 3 4 5\n4 Statistics lecture is interesting. 1 2 3 4 5\n5 Statistics is easy to understand. 1 2 3 4 5\n6 | like statistics. 1 2 3 4 5\n7 | like to deal with data. 1 2 3 4 5\n8 | think statistics lecture is important. 1 2 3 4 5\n9 Statistics is important in research. 1 2 3 4 5\n10 Statistics lecture is important. 1 2 3 4 5\n11 | like to do statistical analysis. 1 2 3 4 5\n12 | carry statistics books where ever | go. 1 2 3 4 5\n\n\nThe output of the LLM is quite comparable to that of OCR (after several refresh/regeneration though). Another notable difference is the absence of the Likert scale from 1 to 5 from the output. You may try using other images containing text. From my testing, it occasionally comments on or generates non-existent text.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#interpreting-a-result-table",
    "href": "ch4.html#interpreting-a-result-table",
    "title": "\n4  Vision\n",
    "section": "\n4.8 Interpreting a Result Table",
    "text": "4.8 Interpreting a Result Table\nNow, we try to ask VLMs to interpret statistical result tables, which are commonly found in academic journals. Let’s try with this table from https://wnarifin.github.io/2024-r-conf-my-pres-R/#29,\n\n\nA sample statistical result table.\n\nUsing llama3.2-vision, we ask it to interpret the table while providing it with some contexts,\n\nq_text = \"\nInterpret the statistical result as given in the table.\nThis was the result for a multiple logistic regression analysis for\ndetermining the associated factors of coronary artery disease.\n\"\nquery(q_text, \"llama3.2-vision\", images = \"img/tbl1.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 123)) |&gt; cat()\n\n\n\nThe table presents the results of a multiple logistic regression analysis examining the associated factors of coronary artery disease (CAD). The variables included are DBP (diastolic blood pressure) and gender.\n\n**Step 1: Understanding the Variables**\n\n*   **DBP**: Diastolic Blood Pressure, measured in mmHg.\n*   **Gender**: Categorical variable indicating male or female.\n\n**Step 2: Interpreting Odds Ratios (OR)**\n\n*   The odds ratio (OR) is a measure of association between an exposure and an outcome. An OR greater than 1 indicates an increased risk, while an OR less than 1 indicates a decreased risk.\n*   For DBP, the OR is 1.641 with a 95% confidence interval (CI) ranging from 1.055 to 4.935. This suggests that for every 10mmHg increase in diastolic blood pressure, there is an approximately 64% increase in the odds of developing CAD.\n*   For Gender, the OR is 2.238 with a 95% CI ranging from 1.055 to 4.935. This indicates that males have approximately twice the odds of developing CAD compared to females.\n\n**Step 3: Interpreting P-Values**\n\n*   The p-value represents the probability of observing the results (or more extreme) assuming there is no real effect. A small p-value (typically &lt;0.05) suggests statistical significance.\n*   For both DBP and Gender, the p-values are less than 0.001, indicating that these factors are statistically significant predictors of CAD.\n\n**Conclusion**\n\nThe analysis reveals that higher diastolic blood pressure and male gender are associated with an increased risk of developing coronary artery disease. These findings suggest that controlling blood pressure and promoting gender equality in health care may be important strategies for preventing CAD.\n\n\nIn this case, it performed quite well when you give it relevant contexts, although it got the lower limit of 95% CI and _P_value for DBP wrong.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#performing-ocr-on-a-result-table",
    "href": "ch4.html#performing-ocr-on-a-result-table",
    "title": "\n4  Vision\n",
    "section": "\n4.9 Performing OCR on a Result Table",
    "text": "4.9 Performing OCR on a Result Table\nThen, we ask llama3.2-vision to read the table and return the text with formatting. This is more challenging than it involves recognizing the lines and arrangement of cells in the table.\n\nquery(\"CAREFULLY read the stastical result table and return the text in Markdown format. Keep the formatting in the table.\", \n      \"llama3.2-vision\", images = \"img/tbl1.png\",\n      output = \"text\", screen = FALSE,\n      model_params = list(seed = 999)\n      ) |&gt; cat()\n\n\n\n### Associated Factors of Coronary Artery Disease (n = 200)\n\n| **Factors** | **b** | **SE** | **Adj. OR** | **95% CI** | **z-s** |\n| --- | --- | --- | --- | --- | --- |\n| DBP (by 10mmHg) | 0.495 | 0.146 | 1.641 | 1.24, 2.21 | 3.38 |\n| Gender (Male vs Female) | 0.806 | 0.391 | 2.238 | 1.055, 4.935 | 2.06 |\n\n### Notes\n\n*   OR = odds ratio\n*   SE = standard error\n\n\nIt scanned the table quite well, except it missed the P-values. From the scanned text, you can get well-formatted text if it is processed as Markdown,\n\n\nAssociated Factors of Coronary Artery Disease (n = 200)\n\n\n\n\n\n\n\n\n\n\n\nFactors\nb\nSE\nAdj. OR\n95% CI\nz-s\n\n\n\nDBP (by 10mmHg)\n0.495\n0.146\n1.641\n1.24, 2.21\n3.38\n\n\nGender (Male vs Female)\n0.806\n0.391\n2.238\n1.055, 4.935\n2.06\n\n\n\n\nNotes\n\n\nOR = odds ratio\nSE = standard error\n\n\nas compared to the original table,\n\n\nThe original statistical result table.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#deep-dive-how-it-works",
    "href": "ch4.html#deep-dive-how-it-works",
    "title": "\n4  Vision\n",
    "section": "\n4.10 Deep-dive: How it works",
    "text": "4.10 Deep-dive: How it works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#references",
    "href": "ch4.html#references",
    "title": "\n4  Vision\n",
    "section": "References",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch4.html#footnotes",
    "href": "ch4.html#footnotes",
    "title": "\n4  Vision\n",
    "section": "",
    "text": "The code to generate the original plot uses ggplot2 (https://wnarifin.github.io/2024-r-conf-my-pres-R/#39).↩︎",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Vision</span>"
    ]
  },
  {
    "objectID": "ch5.html",
    "href": "ch5.html",
    "title": "5  Embedding Generation",
    "section": "",
    "text": "5.1 Preliminaries\nEmbedding is a crucial component in LLMs. It that converts words into numerical vectors to preserve semantic meaning, capture relationships, and enable contextual understanding for the machines. It can be used in retrieval augmented generation (RAG, see: https://ollama.com/blog/embedding-models) and supervised learning (see: https://jbgruber.github.io/rollama/articles/text-embedding.html).\nIn this chapter, we will learn about how to transform raw text data into numerical representations and we will investigate the role of similarity indices in the context of embeddings.\nLoad these packages,\nlibrary(rollama)\nlibrary(purrr)\nlibrary(tibble)\nMake sure that your Ollama is running,\nping_ollama()  # ensure it's running\n\n▶ Ollama (v0.5.1) is running at &lt;http://localhost:11434&gt;!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding Generation</span>"
    ]
  },
  {
    "objectID": "ch5.html#understanding-numerical-embedding",
    "href": "ch5.html#understanding-numerical-embedding",
    "title": "5  Embedding Generation",
    "section": "\n5.2 Understanding numerical embedding",
    "text": "5.2 Understanding numerical embedding\n\n5.2.1 Text to embedding\nThe example and text were taken from https://ollama.com/blog/embedding-models. For our understanding, let’s retrieve back the relevant text (i.e. related in semantic meaning).\nWe convert the strings into vectors via nomic-embed-text model.\n\ndocs = c(\n  \"Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels\",\n  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n  \"Llamas are vegetarians and have very efficient digestive systems\",\n  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\"\n)\nnums = embed_text(docs, \"nomic-embed-text\")\n\nnomic-embed-text generates 768 dimensions for each of the six text strings.\n\ndim(nums)\n\n[1]   6 768\n\nhead(nums)\n\n\n  \n\n\n\nFor example “Llamas are members of the camelid family meaning they’re pretty closely related to vicunas and camels” becomes\n\ndocs[1]\n\n[1] \"Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels\"\n\nnums[1,] |&gt; round(2) |&gt; as.numeric()\n\n  [1]  0.55  0.35 -3.61 -1.24  0.44 -0.51 -1.70 -1.54 -0.69 -0.37 -0.55  1.28\n [13]  1.49  0.42  0.38 -0.48 -1.03 -0.55  1.03  0.51 -1.85  0.79  0.66 -0.23\n [25]  1.22 -0.16 -0.60  0.77  0.24 -0.29  2.44 -0.14  0.81 -0.52  0.01  0.68\n [37]  1.52  0.39  1.84  0.24  1.31 -0.22  0.21  0.55  0.74 -0.52  0.68  0.90\n [49]  2.03 -0.09  1.20 -0.43 -0.87  0.15  0.82  0.08  0.80  0.08  0.28 -0.71\n [61]  1.68 -0.08 -0.38  1.99  0.61 -0.68 -1.26  0.32 -0.54  0.12  0.54 -0.06\n [73]  1.10 -0.13 -0.19  0.93  1.12 -0.21 -0.38 -0.74  1.82  0.35  0.59  0.13\n [85]  0.92  0.27 -1.77 -0.21 -0.19  0.55  0.13  0.53  1.44  0.67 -1.01  1.22\n [97] -0.89  0.13 -0.51 -0.81 -0.86 -0.46  1.22  0.18  0.79 -0.03  0.53 -0.62\n[109] -0.27 -0.41 -1.52  0.06 -0.12 -0.45 -1.11 -0.97  0.78 -0.89  0.82  0.72\n[121] -0.60 -0.92 -0.26  0.80  0.43  0.09 -0.25  0.30  0.97 -0.95 -0.44 -0.53\n[133]  0.10 -0.47  0.34 -0.03  0.00 -0.28  0.63  0.67  0.18  1.06  0.47 -0.71\n[145]  0.36 -0.83  0.80 -0.93 -1.01 -0.55  0.27  0.95 -0.48  0.33 -0.28 -0.67\n[157] -0.43 -0.36 -0.28  0.05  0.45 -0.21  0.21 -0.11 -0.42 -0.55  1.75  0.64\n[169]  1.62  1.62 -1.39 -0.39 -0.04 -0.07  1.07 -0.38  0.26 -0.79 -0.18 -0.67\n[181] -0.31 -1.42 -0.11  1.34 -0.86  0.10 -0.09 -0.22 -1.07 -1.20 -0.72  1.76\n[193]  0.73 -1.23 -0.28 -0.20  0.08 -0.37  0.21 -0.91 -1.09  0.18 -1.85  0.49\n[205] -1.07  0.63 -0.15 -1.21 -0.49  0.81  0.48 -0.30 -0.40  0.02 -0.49 -0.82\n[217] -0.11 -0.37 -0.40  1.24  0.58  0.79  1.28 -0.79  1.58  0.19 -1.26  0.43\n[229] -0.61  0.23  0.20 -0.06  1.42 -0.31  0.66  0.14  0.49  0.46 -0.16 -0.16\n[241] -0.43 -0.61 -0.14 -0.31 -1.36 -0.11  0.35 -0.97 -0.37  1.70  0.30 -0.95\n[253]  0.62  0.17  0.41 -0.09 -0.21 -0.59  0.04 -0.21  0.36 -0.81  0.77 -0.59\n[265] -0.54 -0.65  0.15 -0.07 -0.24 -0.82 -0.51  0.83  1.12 -0.43  0.11 -0.09\n[277] -0.30 -0.49 -0.62 -0.20  1.49 -1.27 -0.64 -0.50 -0.05  0.72  0.98  0.30\n[289]  0.28 -0.09  0.47  0.78  0.17  1.53  0.73 -0.21  2.04  0.64 -0.25  0.48\n[301] -0.10  0.00  1.19  0.26  0.09  0.07  0.22 -0.68  0.99 -0.71 -0.27 -0.10\n[313] -0.33  0.14 -0.19  1.00 -0.06  0.20  1.07  1.08 -0.10 -1.09 -0.42 -1.16\n[325]  0.88  0.27 -0.69  0.43  0.06 -1.00 -0.22  1.15  1.23  0.12 -0.23  1.14\n[337] -1.10 -0.27 -0.31  0.45  0.73 -0.49  1.64 -0.83 -1.28  0.60 -0.42 -0.11\n[349]  0.43 -0.79 -0.63  0.13  0.41  0.34  0.60 -0.93  0.31 -0.15  0.47 -0.15\n[361]  0.98 -0.15 -0.18 -0.51  0.54 -0.17  0.36  0.29  0.93 -0.32 -0.63  0.62\n[373] -0.73  0.03  0.75  0.58 -1.01 -0.94 -1.89 -0.68  0.45 -0.13  0.39  1.16\n[385]  0.54 -0.04  0.58 -0.66  0.05  0.45 -0.06 -0.18 -1.43 -0.69  0.06 -0.55\n[397]  1.61  0.05 -0.82  1.60 -0.89 -0.79  0.06  0.11  0.37  0.94 -0.51 -0.93\n[409]  1.22  0.13  1.02  0.18  0.94 -0.19 -0.28  2.08 -0.07  0.59 -0.45 -0.11\n[421] -0.05  0.97 -0.13  0.75  0.09  0.27 -0.13  0.50 -0.05 -0.55  0.22  1.17\n[433]  1.48  0.03  0.50 -0.58 -0.08  0.93  0.35 -0.08  1.15 -0.12 -0.66 -0.60\n[445]  0.79  1.91  0.71 -0.43  0.08  0.98 -0.08  0.72  0.57 -0.11  1.30 -0.57\n[457] -0.22 -0.51  1.62  0.37  0.84  0.17 -0.46  0.95  0.32 -0.71  0.50  0.38\n[469] -0.35  0.82 -0.60  0.69 -0.37 -0.42 -1.42 -0.62 -0.31  0.32  0.87  0.71\n[481]  0.78  0.30 -1.00 -2.85  0.26  0.94  0.61 -0.40 -0.32  0.42  0.37 -0.58\n[493]  0.68  0.35  0.01 -1.25 -0.15  0.23  2.00  0.59  0.42  1.33 -0.25 -0.63\n[505]  1.01 -0.19  0.10 -1.10 -0.72  0.41 -1.30  0.21 -0.99  0.60  1.20 -1.17\n[517]  0.92  0.29 -0.30  0.08 -0.34 -0.63  0.03 -0.46 -0.76 -0.25  0.00 -0.91\n[529] -0.04 -0.18 -0.63 -0.25 -0.45 -0.06 -0.54 -0.50  0.28  1.48  0.83 -0.20\n[541]  0.76 -0.91 -0.36  0.55  0.07  0.26 -0.48 -0.38  0.16  0.25 -0.37 -1.46\n[553] -0.11 -1.15 -0.05 -0.94 -0.13  0.12 -0.73 -0.16  0.19  0.35  0.43 -0.54\n[565] -0.79 -0.88  0.00  0.05  0.23  0.31  0.41 -1.34 -0.17  0.14  0.88 -0.58\n[577]  0.10 -0.17 -0.69 -0.73  0.01 -1.89 -1.06  0.26 -0.38  0.52 -0.70 -0.30\n[589] -0.20 -0.63  0.51  0.92 -1.04 -0.87  0.08 -0.57  0.27 -0.76 -0.50 -1.14\n[601]  0.85 -1.10  1.52  0.07 -1.04  0.53  0.09  0.96  0.21 -0.36  0.75  0.47\n[613] -0.39 -0.37  0.47 -0.34  0.23 -1.42 -1.14  0.16 -0.74 -0.69 -0.23  0.30\n[625] -0.37  0.70 -2.32 -0.24  0.48  0.41 -1.29 -0.18 -0.72 -1.15 -0.28 -0.04\n[637] -1.45 -0.37 -0.20  0.79  0.52 -0.54 -0.57  1.03  1.04 -0.67  0.09  0.21\n[649]  0.47  0.14  1.91  1.43  0.86  0.73  0.17 -0.54  0.59 -1.44 -1.07 -0.21\n[661] -0.33  1.20 -0.63  0.60 -0.25  0.15  0.02 -1.13 -1.60  0.86  0.39 -0.64\n[673] -0.10 -0.79 -0.13 -0.39  0.34  0.33  1.76 -0.16  0.25 -0.32 -0.72 -0.23\n[685] -0.60 -0.72  0.93 -1.13  0.09 -1.17 -0.48 -1.45  0.00 -0.46  0.17 -0.04\n[697]  0.24 -0.01 -0.32  1.57 -0.70  0.98  0.21  1.02  0.46 -0.67  0.06  1.69\n[709]  0.18 -0.28 -1.10  1.07 -0.46  1.69  1.07 -0.61 -0.71 -0.06  0.01  0.22\n[721]  0.76 -0.66  0.67 -0.29 -0.45 -1.13  0.95 -2.16 -0.04 -0.69  0.59 -0.21\n[733] -0.30  0.50 -0.30  0.10 -0.79  0.28  0.66 -0.06 -1.20 -0.13 -0.20  0.40\n[745]  1.09 -1.69 -0.52  1.23  0.20  0.07  0.49 -0.82  0.07 -0.24  0.31  0.29\n[757] -0.92  0.67  0.94 -0.24  1.26 -0.13  0.99  0.30 -1.40 -1.10 -2.36 -0.93\n\n\nand “Llamas are vegetarians and have very efficient digestive systems” becomes\n\ndocs[5]\n\n[1] \"Llamas are vegetarians and have very efficient digestive systems\"\n\nnums[5,] |&gt; round(2) |&gt; as.numeric()\n\n  [1]  0.03  1.86 -3.79 -0.37  0.44 -0.65 -1.73 -0.90 -1.30  0.02  0.49  1.28\n [13]  1.11  0.85  0.59 -0.22 -0.32 -1.05  0.78  0.74 -1.58  1.00  0.29 -0.65\n [25]  1.49  0.18 -0.96  2.20  0.39 -0.15  1.04 -1.08 -0.28 -0.06  0.43  0.43\n [37]  2.19  0.17  1.59 -0.63  1.07  0.15  0.77 -0.10  0.67  0.19  0.96  0.30\n [49]  0.89 -1.15  0.03 -0.94 -1.12  0.32  1.27 -0.63  0.05 -0.24 -0.03 -1.02\n [61]  1.16 -0.15 -0.85  2.18  0.04 -1.57 -0.73  0.09 -0.44 -0.05  0.96 -0.93\n [73]  0.28 -0.80  0.90  0.74  0.05 -0.05  0.02 -0.33  0.85  0.33  0.88 -0.66\n [85]  1.09 -0.12 -0.44 -1.13 -0.04  0.52  0.20  1.14  1.54  0.67 -1.19  0.92\n [97] -0.88  0.31 -0.32 -0.29 -0.39 -0.46  1.09 -0.18  1.15 -0.25  1.34 -0.73\n[109] -0.49 -0.02 -0.35  0.17  0.07 -0.23 -0.23 -1.14  1.83 -0.66  0.88  0.70\n[121] -0.14 -0.28  0.54  0.17  0.61  0.53 -0.27  0.08  0.52 -0.86 -0.40  0.49\n[133]  0.30 -1.06  0.07 -0.08 -0.64 -0.35  0.14  0.29  0.81  0.50  0.42  0.09\n[145] -0.12 -1.55  0.70 -0.78 -0.50  0.29  0.60  1.66  0.31  0.96 -0.52 -0.77\n[157] -0.28 -0.17 -0.27 -0.12  0.69  0.49  0.08  0.45 -0.15 -0.19  2.00  0.59\n[169]  0.84  1.58 -1.30 -0.53 -0.14  0.07  0.67 -0.69  0.18 -1.03  0.85 -0.45\n[181]  0.14 -0.83  0.46  1.48 -0.40 -0.07  0.40 -0.24 -1.41 -1.59 -0.71  1.83\n[193]  0.37 -1.35 -0.37  0.37  0.15 -0.82  0.35 -1.28 -1.23 -0.74 -2.09  0.30\n[205] -1.20  0.37 -0.52 -0.79 -0.62  0.39  0.63 -0.19 -0.09  0.10  0.02 -0.35\n[217] -0.91  0.05  0.33  0.96  1.01  0.31  0.86 -0.67  1.21 -0.42 -1.48 -0.18\n[229] -1.22  0.38  0.45 -0.11  1.46 -0.38  0.14  0.20  0.45  0.71  0.68 -0.61\n[241] -0.31 -0.56 -0.07  0.18 -0.79 -0.06  0.32 -1.06 -0.54  1.60  0.99 -0.94\n[253]  0.38 -0.38  0.00 -0.13  0.10 -0.73  1.08 -0.65  0.36 -0.89  0.30  0.32\n[265] -0.05 -0.71  0.33  0.60 -0.29 -1.26 -0.31 -0.33  0.77 -0.17  0.11  0.23\n[277] -0.44 -0.52 -0.12  0.27  0.92 -1.72 -0.91  0.51 -0.24  0.62  0.65 -0.28\n[289] -0.07 -0.30  0.38  0.92 -0.23  1.58 -0.21  0.15  1.97  0.08 -0.07 -0.26\n[301] -0.09  0.28  0.98  0.88 -0.22  0.50  0.59 -0.40  1.02 -0.34  0.11  0.12\n[313] -0.57 -0.02 -0.45  0.13 -0.22  0.38  1.51  1.08  0.57 -1.07 -0.69 -1.50\n[325]  0.66  0.42 -0.19  0.45  0.29 -1.02  0.16  1.35  1.13 -0.49 -0.64  0.61\n[337] -0.50  0.14 -0.56  0.20  0.01 -0.73  1.42 -0.68 -1.00  0.04 -0.40 -0.78\n[349]  0.16 -0.46 -0.04  0.06  0.61  0.96 -0.40 -0.57  0.41 -0.60  0.73  0.07\n[361]  0.56 -0.63  0.04 -0.65  0.68  0.42  0.26 -0.68  0.32 -0.17 -0.32 -0.06\n[373] -0.94  0.32  0.36  0.47 -1.17 -0.89 -1.50 -0.21  1.06 -0.81 -0.19  0.42\n[385]  0.14 -0.76  0.49 -0.56  0.08  0.68  0.33 -1.21 -1.43 -0.03 -0.03 -0.44\n[397]  2.03 -0.58  0.12  1.87 -0.46 -0.85  0.05 -0.09  0.00  0.62 -0.25 -0.70\n[409]  0.61 -0.15  1.03  1.07  1.02 -0.37 -0.67  1.28 -0.30 -0.28 -0.46 -0.33\n[421] -0.32  1.06  0.19  0.39 -0.15  0.29  0.00  1.53  0.06 -0.65  0.48  0.23\n[433]  1.36  0.54  0.48  0.53 -1.32  0.49  0.72  0.99  0.78 -0.36 -0.81 -1.56\n[445]  0.59  1.46  0.62 -0.98 -0.06 -0.43  1.22  1.28  0.52  0.36  1.53 -0.15\n[457] -0.78 -0.45  0.95  0.29 -0.13  0.05 -0.75  0.52 -0.81 -0.10  0.95  0.95\n[469]  0.24  0.54 -0.27  0.56 -0.24 -0.59 -1.32 -0.38 -0.62  0.43  0.67  0.94\n[481]  0.65 -0.14 -0.46 -1.91  0.59  0.50  0.95 -1.03  0.23  0.45 -0.47  0.46\n[493]  0.44 -0.09 -0.05 -0.79  0.18 -0.31  0.77  0.67 -0.28  0.34  0.10 -0.85\n[505]  0.61 -0.43  0.18 -0.88 -0.74  0.87 -0.61  0.67 -0.38  0.55  1.30 -1.04\n[517]  1.02  0.38 -0.85  0.40  0.38 -0.28  0.33 -0.11 -0.77  0.00  0.10 -0.90\n[529]  0.30 -0.14 -0.32 -0.34 -0.22  0.57 -1.03 -0.40  0.25  1.95  0.80 -0.56\n[541]  0.93 -0.71 -0.47  0.42  0.04  0.22 -1.44 -0.53 -0.77 -0.25 -1.06 -1.64\n[553]  0.58 -1.38  0.81 -0.53 -0.23  0.93 -0.37 -0.58  0.79  0.69  1.22 -0.92\n[565] -0.76 -1.46  0.03 -0.33  0.35  0.18  0.53 -0.37  0.54  0.56  0.57 -0.86\n[577] -0.09 -0.38 -0.98 -0.73  0.57 -1.79 -0.99  0.02  0.03  0.61 -0.20 -0.12\n[589]  0.14 -0.64  0.44  0.76 -0.66 -0.87  0.41 -0.27 -0.43 -0.89 -0.75 -1.21\n[601]  0.61 -0.83  0.94 -0.64 -0.60  0.54 -0.37  1.12 -0.09 -0.41  0.14  0.69\n[613] -0.85 -1.04  0.58 -0.62  0.04 -0.99 -0.83  0.42 -0.80 -1.02  0.49 -0.26\n[625] -0.08 -0.08 -2.10 -0.17  0.01 -0.05 -1.10  0.33 -0.30 -1.38 -0.98 -0.56\n[637] -0.99 -0.31  0.43  0.34  0.80 -0.02  0.00  0.55  0.89  0.59  0.36 -0.20\n[649]  0.62 -0.57  1.41  1.53  1.32  0.15  0.57  0.04  0.04 -0.54 -1.09 -0.86\n[661]  0.33  0.89  0.28  1.39  0.48 -0.36 -0.34 -1.13 -1.87  1.21  0.62 -0.41\n[673] -0.30 -0.70  0.23 -0.01  0.57 -0.02  1.55  0.03  0.44 -0.76 -0.37 -0.19\n[685]  0.03 -1.28  1.03 -1.22  0.47 -0.57 -0.02 -1.39 -0.37 -0.11 -0.11 -0.44\n[697] -0.43  0.40 -0.09  1.15 -0.40  1.21  0.41  1.58  0.27 -1.64  0.25  1.16\n[709]  0.01  0.16 -0.66  0.40 -0.89  1.90  1.37 -0.51 -1.52 -0.78  0.18  0.24\n[721]  1.25  0.03  0.10 -0.38 -0.06 -0.99  1.14 -1.95  0.17 -0.55  0.94 -0.56\n[733] -0.55  0.83 -0.31  0.18 -0.83  0.40 -0.15  0.24 -1.01  0.41 -0.23  0.39\n[745]  1.50 -0.71 -0.62  0.51  1.06  0.04  0.83 -1.67 -0.98 -0.16 -0.30  0.42\n[757] -0.29  0.82  0.86  0.11  1.04  0.23  1.23 -0.35 -1.74 -1.36 -1.54 -0.83\n\n\nWe will also look at the use of numerical embeddings later in sec-classification in the context of text classification.\n\n5.2.2 Similarity index\nNow, once we have a related query, how can we find related text strings (based on the numerical values)?\n\nq_text1 = \"What animals are llamas related to?\"\nq_text2 = \"How long llamas can live?\"\n\nFor the first query, “What animals are llamas related to?”, we transform it into embedding, and combine it with the embeddings that we obtained earlier,\n\nq_num = embed_text(q_text1, \"nomic-embed-text\") # turn into numerical vector\nmat = rbind(as.numeric(q_num), as.matrix(nums)) # bind query text with existing texts\nmat = t(mat) # need to change rows to columns for cosine function in lsa\nmat |&gt; round(2) |&gt; head()\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\ndim_1  0.33  0.55  0.65  0.97  0.05  0.03  0.55\ndim_2  0.25  0.35 -0.02  1.28  0.73  1.86 -0.13\ndim_3 -3.44 -3.61 -4.03 -3.66 -3.90 -3.79 -3.93\ndim_4 -0.93 -1.24 -0.80 -0.77 -1.20 -0.37 -1.14\ndim_5  0.68  0.44  0.33 -1.24 -0.30  0.44 -0.65\ndim_6  0.04 -0.51  0.17  0.68  0.72 -0.65  1.10\n\n\nThen, calculate cosine similarity index between our query and other text strings. We use cosine() from lsa package. We combine the numbers with text to make sense of the link between text strings and their corresponding embeddings,\n\nlibrary(lsa)  # for cosine similarity\ncos_sim = cosine(mat) # cosine similarities between the vectors\n\ndf = data.frame(Text = c(q_text1, docs), Similarity = cos_sim)  # combined into df\ndf = df |&gt; dplyr::arrange(desc(Similarity.1))  # arrange with respect to Similarity.1\ndf                                             # i.e. our query\n\n\n  \n\n\n\n\n\n# A tibble: 7 × 8 \n  Text          Similarity.1 Similarity.2 Similarity.3 Similarity.4 Similarity.5 \n  &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; \n1 What animals…        1            0.899        0.759        0.725        0.777 \n2 Llamas are m…        0.899        1            0.754        0.715        0.749 \n3 Llamas are v…        0.800        0.816        0.753        0.718        0.802 \n4 Llamas weigh…        0.777        0.749        0.763        0.811        1     \n5 Llamas were …        0.759        0.754        1            0.721        0.763 \n6 Llamas can g…        0.725        0.715        0.721        1            0.811 \n7 Llamas live …        0.724        0.723        0.786        0.793        0.798 \n# ℹ 2 more variables: Similarity.6 &lt;dbl&gt;, Similarity.7 &lt;dbl&gt;\n\n\nwhich shows that “Llamas are members of the camelid family meaning they’re pretty closely related to vicunas and camels” is highly similar (similarity = 0.899).\nFor the second query, “How long llamas can live?”\n\nq_num = embed_text(q_text2, \"nomic-embed-text\")\nmat = rbind(as.numeric(q_num), as.matrix(nums))\nmat = t(mat)\nmat |&gt; round(2) |&gt; head()\n\n       [,1]  [,2]  [,3]  [,4]  [,5]  [,6]  [,7]\ndim_1  0.88  0.55  0.65  0.97  0.05  0.03  0.55\ndim_2 -0.11  0.35 -0.02  1.28  0.73  1.86 -0.13\ndim_3 -3.62 -3.61 -4.03 -3.66 -3.90 -3.79 -3.93\ndim_4 -0.94 -1.24 -0.80 -0.77 -1.20 -0.37 -1.14\ndim_5 -0.50  0.44  0.33 -1.24 -0.30  0.44 -0.65\ndim_6  0.42 -0.51  0.17  0.68  0.72 -0.65  1.10\n\n\n\ncos_sim = cosine(mat)\n\ndf = data.frame(Text = c(q_text2, docs), Similarity = cos_sim)\ndf = df |&gt; dplyr::arrange(desc(Similarity.1))\ndf\n\n\n  \n\n\n\n\n\n# A tibble: 7 × 8 \n  Text          Similarity.1 Similarity.2 Similarity.3 Similarity.4 Similarity.5 \n  &lt;chr&gt;                &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt; \n1 How long lla…        1            0.696        0.742        0.782        0.768 \n2 Llamas live …        0.915        0.723        0.786        0.793        0.798 \n3 Llamas can g…        0.782        0.715        0.721        1            0.811 \n4 Llamas weigh…        0.768        0.749        0.763        0.811        1     \n5 Llamas were …        0.742        0.754        1            0.721        0.763 \n6 Llamas are v…        0.726        0.816        0.753        0.718        0.802 \n7 Llamas are m…        0.696        1            0.754        0.715        0.749 \n# ℹ 2 more variables: Similarity.6 &lt;dbl&gt;, Similarity.7 &lt;dbl&gt;\n\n\nit rightly points out that “Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old” is highly similar (similarity = 0.915).\nWhat you have learned in this sub-section will be relevant for you to understand RAG in sec-rag.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding Generation</span>"
    ]
  },
  {
    "objectID": "ch5.html#deep-dive-how-it-works",
    "href": "ch5.html#deep-dive-how-it-works",
    "title": "5  Embedding Generation",
    "section": "\n5.3 Deep-dive: How it works",
    "text": "5.3 Deep-dive: How it works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding Generation</span>"
    ]
  },
  {
    "objectID": "ch5.html#references",
    "href": "ch5.html#references",
    "title": "5  Embedding Generation",
    "section": "References",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Embedding Generation</span>"
    ]
  },
  {
    "objectID": "ch6.html",
    "href": "ch6.html",
    "title": "6  Text Classification",
    "section": "",
    "text": "6.1 Deep-dive: How it works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Text Classification</span>"
    ]
  },
  {
    "objectID": "ch7.html",
    "href": "ch7.html",
    "title": "7  Retrieval Augmented Generation",
    "section": "",
    "text": "7.1 About\nLLMs generally have limited knowledge; they only know what was ‘taught’ to them based on their training texts. Any new information not included in these texts is beyond their generation capability. It is impractical to retrain LLMs every time we want them to generate text with new information. A simple solution is to provide the necessary information directly in our queries. However, if we do not know exactly what we are looking for and the required information might be from a thick technical manual, a more practical approach is Retrieval-Augmented Generation (RAG). In this chapter, we will explore how to perform RAG by pairing our knowledge in the form of a vector database with LLMs.\nIn this chapter, we will explore how to perform RAG by pairing our knowledge in the form of a vector database with LLMs. So far, most resources I have found are in Python, and I aim to achieve this using R.\n[deepseek-r1:7b]\nRAG is an approach that enhances LLMs by augmenting their capabilities with document retrieval systems. This integration allows for more accurate, relevant, and contextually informed responses compared to relying solely on the model’s internal knowledge.\nHow It Works:\nKey Components:\nWhy Use RAG?\nUse Cases:\nRAG differs from fine-tuning (which involves retraining the model) or prompt engineering (which relies on phrasing) by adding external context without altering the model itself. RAG effectively bridges the gap between LLM general knowledge and specific, detailed information, offering more accurate and relevant responses through augmented input.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "ch7.html#about",
    "href": "ch7.html#about",
    "title": "7  Retrieval Augmented Generation",
    "section": "",
    "text": "Document Retrieval: When a query is received, RAG first retrieves relevant documents or passages from an external source (e.g., a database) using techniques like embedding-based search.\n\nContext Integration: The retrieved documents are then provided to the LLM as context, enabling the model to generate responses informed by both its internal knowledge and the specific retrieved information.\n\n\n\n\nEmbedding-Based Retrieval: Documents are converted into vector representations, allowing efficient matching with query vectors through similarity measures.\n\nEfficient Search: Techniques like vector databases or similarity search quickly identify relevant documents, ensuring timely retrieval without significant computational overhead.\n\n\n\n\nEnhanced Accuracy: By incorporating up-to-date or specific data, RAG reduces reliance on the model’s potentially outdated or generic knowledge.\n\nHandling Specialized Queries: RAG is particularly effective for domain-specific questions requiring detailed information beyond the model’s training data.\n\nEfficiency: Focusing the model on a subset of relevant documents can optimize performance and reduce computational demands.\n\n\n\nCustomer service chatbots needing access to product details or customer records.\nHealthcare applications where accurate medical guidelines are crucial.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "ch7.html#basic-rag-with-chroma",
    "href": "ch7.html#basic-rag-with-chroma",
    "title": "7  Retrieval Augmented Generation",
    "section": "\n7.2 Basic RAG with Chroma",
    "text": "7.2 Basic RAG with Chroma\nWe learned about text embedding and how we can utilize it to find texts related to our query in sec-embedding. Now, in the context of RAG, let’s go back to the text in sec-text-embedding (https://ollama.com/blog/embedding-models). Suppose that now, we want to ask a query, find the related text, then let an LLM answer our query based on the information (retrieved text) given to it. We will use Chroma (https://www.trychroma.com/) and rchroma (https://cynkra.github.io/rchroma/) for that. Make sure you have a working Chroma installation (client-server mode https://docs.trychroma.com/docs/run-chroma/client-server, docker https://docs.trychroma.com/production/containers/docker) accessible at localhost:8000 (e.g. http://localhost:8000/api/v2). We will also used our beloved rollama for generating embeddings and text generation using LLMs of our choice.\nLoad these packages,\n\nlibrary(rchroma)\nlibrary(rollama)\n\nMake sure that your Ollama is running,\n\nping_ollama()  # ensure it's running\n\n▶ Ollama (v0.6.7) is running at &lt;http://localhost:11434&gt;!\n\n\nMake sure Chroma server is running. In my case, I am using the client-server mode, and I check its status at http://localhost:8000/api/v2. If you are running Chroma via Docker, you may check by chroma_docker_running().\nWe create a Chroma client connection,\n\nclient = chroma_connect()\n\nand check its status using heartbeat(),\n\nheartbeat(client)\n\n[1] 1.747906e+18\n\n\n\n7.2.1 Setting up ChromaDB collection\nNext, we create a ChromaDB collection named “my_collection”,\n\ncreate_collection(client, \"my_collection\")\n\n$id\n[1] \"4d2a6726-2c57-4b20-bb26-f1a52dc2079f\"\n\n$name\n[1] \"my_collection\"\n\n$configuration_json\n$configuration_json$hnsw\n$configuration_json$hnsw$space\n[1] \"l2\"\n\n$configuration_json$hnsw$ef_construction\n[1] 100\n\n$configuration_json$hnsw$ef_search\n[1] 100\n\n$configuration_json$hnsw$max_neighbors\n[1] 16\n\n$configuration_json$hnsw$resize_factor\n[1] 1.2\n\n$configuration_json$hnsw$sync_threshold\n[1] 1000\n\n\n$configuration_json$spann\nNULL\n\n$configuration_json$embedding_function\nNULL\n\n\n$metadata\nNULL\n\n$dimension\nNULL\n\n$tenant\n[1] \"default_tenant\"\n\n$database\n[1] \"default_database\"\n\n$log_position\n[1] 0\n\n$version\n[1] 0\n\n\nChroma and rchroma currently uses “l2” similarity index as the default distance function.\nIn order to add text documents to “the collection”my_collection”, we will use add_documents() function. To properly use the function, it involves several steps. First, we prepare the text, for example we use text strings from https://ollama.com/blog/embedding-models,\n\ndocuments = c(\n  \"Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels\",\n  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n  \"Llamas are vegetarians and have very efficient digestive systems\",\n  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\"\n)\n\nand convert the text into text embeddings via nomic-embed-text embedding model,\n\nembeddings = embed_text(documents, \"nomic-embed-text\")\n\nThe function expects unnamed list of numeric vectors for the embeddings option, therefore we have to convert the embeddings from tibble to unnamed list,\n\nunnamed_embeddings = apply(embeddings, 2, as.numeric) |&gt; \n  split(f = 1:length(documents)) |&gt; unname()\n\nLastly, we add these objects to the function,\n\nadd_documents(\n  client,\n  \"my_collection\",\n  documents = documents,\n  ids = paste0(\"doc\", 1:length(documents)),\n  embeddings = unnamed_embeddings\n)\n\nnamed list()\n\n\nNote that the option ids is compulsory. So we generate the ids as combinations of “doc” and “1:length(documents)” e.g. “doc1”, “doc2”, and so on.\nWe can view configuration details of “my_collection”,\n\nget_collection(client, \"my_collection\")\n\n$id\n[1] \"4d2a6726-2c57-4b20-bb26-f1a52dc2079f\"\n\n$name\n[1] \"my_collection\"\n\n$configuration_json\n$configuration_json$hnsw\n$configuration_json$hnsw$space\n[1] \"l2\"\n\n$configuration_json$hnsw$ef_construction\n[1] 100\n\n$configuration_json$hnsw$ef_search\n[1] 100\n\n$configuration_json$hnsw$max_neighbors\n[1] 16\n\n$configuration_json$hnsw$resize_factor\n[1] 1.2\n\n$configuration_json$hnsw$sync_threshold\n[1] 1000\n\n\n$configuration_json$spann\nNULL\n\n$configuration_json$embedding_function\nNULL\n\n\n$metadata\nNULL\n\n$dimension\n[1] 768\n\n$tenant\n[1] \"default_tenant\"\n\n$database\n[1] \"default_database\"\n\n$log_position\n[1] 0\n\n$version\n[1] 0\n\n\n\n7.2.2 Retrieving information from ChromaDB collection\nBefore we try out basic RAG, let’s look at how to retrieve/query the collection. We set up the query text and its embedding,\n\nquery_text = \"What animals are llamas related to?\"\nquery_embedding = embed_text(query_text, \"nomic-embed-text:latest\") |&gt; as.numeric()\n\nThen, we query “my_collection”. Note we use rchroma::query() because it overlaps with rollama’s query() function,\n\nresult = rchroma::query(\n  client,\n  \"my_collection\",\n  query_embeddings = list(query_embedding),\n  n_results = 3\n)\n\nwhere n_results is the number of results to return per query. We set it to “3”.\nWe can view these three results and their distances,\n\nresult$documents[[1]]\n\n[[1]]\n[1] \"Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels\"\n\n[[2]]\n[1] \"Llamas are vegetarians and have very efficient digestive systems\"\n\n[[3]]\n[1] \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\"\n\nresult$distances[[1]]\n\n[[1]]\n[1] 98.88392\n\n[[2]]\n[1] 194.9566\n\n[[3]]\n[1] 214.2481\n\n\nSpecifically, we are interested in the first result,\n\noutput = result$documents[[1]][[1]]\noutput\n\n[1] \"Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels\"\n\n\n\n7.2.3 Basic RAG\nNow, we integrate the returned query output in our prompt. We can write our basic prompt like this,\n\nq_text = paste(\"Based on the following information:\",\n               output,\n               \"\\nAnswer the query:\",\n               query_text,\n               \"\\nUse only the information provided. Do not reference external knowledge, assumptions, or sources not explicitly included in the given text. If the information is insufficient to answer the query, state that clearly.\")\n# Prompt writing with the help of qwen3:14b\ncat(q_text)\n\nBased on the following information: Llamas are members of the camelid family meaning they're pretty closely related to vicunas and camels \nAnswer the query: What animals are llamas related to? \nUse only the information provided. Do not reference external knowledge, assumptions, or sources not explicitly included in the given text. If the information is insufficient to answer the query, state that clearly.\n\n\nThen, we use the prompt to query the LLM, e.g. llama3.2:3b in our case,\n\nrollama::query(q_text, model = \"llama3.2:3b\",\n      screen = F, output = \"text\") |&gt; cat()\n\nBased on the provided information, it can be concluded that:\n\nLlamas are related to vicunas and camels.\n\nHowever, the information does not provide a comprehensive list of all animals that llamas are related to. It only mentions that they are part of the camelid family, but does not specify other relatives.\n\n\nSo, instead of spitting out the original text “Llamas are members of the camelid family meaning they’re pretty closely related to vicunas and camels”, the LLM synthesize the information and answer the query.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "ch7.html#deep-dive-how-it-works",
    "href": "ch7.html#deep-dive-how-it-works",
    "title": "7  Retrieval Augmented Generation",
    "section": "\n7.3 Deep-dive: How it works",
    "text": "7.3 Deep-dive: How it works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Retrieval Augmented Generation</span>"
    ]
  },
  {
    "objectID": "ch8.html",
    "href": "ch8.html",
    "title": "8  Fine-tuning",
    "section": "",
    "text": "8.1 Deep-dive: How it works\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Fine-tuning</span>"
    ]
  },
  {
    "objectID": "ch9.html",
    "href": "ch9.html",
    "title": "\n9  Miscellaneous Techniques\n",
    "section": "",
    "text": "9.1 Tool Calling\nThese are additional techniques in utilizing local LLMs that I want to learn, explore, and share.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous Techniques</span>"
    ]
  },
  {
    "objectID": "ch9.html#tool-calling",
    "href": "ch9.html#tool-calling",
    "title": "\n9  Miscellaneous Techniques\n",
    "section": "",
    "text": "9.1.1 About\n[deepseek-r1:7b]\nTool calling in the context of Large Language Models (LLMs) refers to a method where the model delegates specific subtasks or data retrieval processes to external tools, databases, or services. This approach allows LLMs to handle complex tasks more efficiently by leveraging specialized resources while maintaining higher-level reasoning capabilities internally.\nKey Points:\n\nDelegation of Tasks: Tool calling enables an LLM to break down intricate problems into manageable parts, delegating complex subtasks to external tools. For example, for a weather forecast request, the model might delegate data retrieval to a reliable meteorological service.\nEfficiency and Resource Management: By offloading tasks that require external knowledge or resources, tool calling allows the LLM to stay focused on higher-level reasoning without being bogged down by internal processing limitations.\nAccess to External Resources: This method often involves accessing databases, APIs, or other systems, which can provide real-time data and specialized information crucial for accurate responses.\nPrivacy Considerations: Safeguards are in place to ensure that access to external tools does not expose sensitive information, maintaining data security and anonymization.\nTask Limitations and Scalability: There is a boundary between tasks the LLM can handle natively and those it must outsource. Tool calling contributes to scalability but may be constrained by processing power and network capabilities when handling multiple complex tasks simultaneously.\n\nIn essence, tool calling enhances an LLM’s functionality by expanding its problem-solving capabilities beyond basic text generation, allowing it to address a broader range of tasks efficiently and effectively.\n\n9.1.2 Models with Tool Calling\nFor Ollama, you can identify LLMs with tool calling capability from the description of the models in the Ollama’s model page, or from the template link on the page. For example, the template page for LLama3.2:3b is https://ollama.com/library/llama3.2/blobs/966de95ca8a6 with the following template text:\n&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\nCutting Knowledge Date: December 2023\n\n{{ if .System }}{{ .System }}\n{{- end }}\n{{- if .Tools }}When you receive a tool call response, use the output to format an answer to the orginal user question.\n\nYou are a helpful assistant with tool calling capabilities.\n{{- end }}&lt;|eot_id|&gt;\n{{- range $i, $_ := .Messages }}\n{{- $last := eq (len (slice $.Messages $i)) 1 }}\n{{- if eq .Role \"user\" }}&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n{{- if and $.Tools $last }}\n\nGiven the following functions, please respond with a JSON for a function call with its proper arguments that best answers the given prompt.\n\nRespond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. Do not use variables.\n\n{{ range $.Tools }}\n{{- . }}\n{{ end }}\n{{ .Content }}&lt;|eot_id|&gt;\n{{- else }}\n\n{{ .Content }}&lt;|eot_id|&gt;\n{{- end }}{{ if $last }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n{{ end }}\n{{- else if eq .Role \"assistant\" }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n{{- if .ToolCalls }}\n{{ range .ToolCalls }}\n{\"name\": \"{{ .Function.Name }}\", \"parameters\": {{ .Function.Arguments }}}{{ end }}\n{{- else }}\n\n{{ .Content }}\n{{- end }}{{ if not $last }}&lt;|eot_id|&gt;{{ end }}\n{{- else if eq .Role \"tool\" }}&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt;\n\n{{ .Content }}&lt;|eot_id|&gt;{{ if $last }}&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n{{ end }}\n{{- end }}\n{{- end }}\n\n9.1.3 Calling R Functions\nFor tool calling, we will use ellmer package, which allows using tool calling of R functions (https://ellmer.tidyverse.org/reference/tool.html and https://ellmer.tidyverse.org/articles/tool-calling.html for details).\nWe start with the first example from https://ellmer.tidyverse.org/reference/tool.html, we need to define the metadata that the model will use to understand when it needs to call the tool,\n\nlibrary(ellmer)\n# define the metadata\ntool_rnorm &lt;- tool(\n  .fun = rnorm,\n  .description = \"Draw numbers from a random normal distribution\",\n  n = type_integer(\"The number of observations. Must be a positive integer.\"),\n  mean = type_number(\"The mean value of the distribution.\"),\n  sd = type_number(\"The standard deviation of the distribution. Must be a non-negative number.\")\n)\ntool_rnorm\n\n&lt;ellmer::ToolDef&gt;\n @ name       : chr \"rnorm\"\n @ fun        : function (n, mean = 0, sd = 1)  \n @ description: chr \"Draw numbers from a random normal distribution\"\n @ arguments  : &lt;ellmer::TypeObject&gt;\n .. @ description          : NULL\n .. @ required             : logi TRUE\n .. @ properties           :List of 3\n .. .. $ n   : &lt;ellmer::TypeBasic&gt;\n .. ..  ..@ description: chr \"The number of observations. Must be a positive integer.\"\n .. ..  ..@ required   : logi TRUE\n .. ..  ..@ type       : chr \"integer\"\n .. .. $ mean: &lt;ellmer::TypeBasic&gt;\n .. ..  ..@ description: chr \"The mean value of the distribution.\"\n .. ..  ..@ required   : logi TRUE\n .. ..  ..@ type       : chr \"number\"\n .. .. $ sd  : &lt;ellmer::TypeBasic&gt;\n .. ..  ..@ description: chr \"The standard deviation of the distribution. Must be a non-negative number.\"\n .. ..  ..@ required   : logi TRUE\n .. ..  ..@ type       : chr \"number\"\n .. @ additional_properties: logi FALSE\n\n\nThen, we must redefine the provider’s base URL to Ollama’s OpenAI compatible endpoint as base_url = \"http://localhost:11434/v1\" according to https://ollama.com/blog/tool-support, specify the local LLM that we want to use, followed by registering the tool.\n\nProvider(base_url = \"http://localhost:11434/v1\")\n\n&lt;ellmer::Provider&gt;\n @ base_url  : chr \"http://localhost:11434/v1\"\n @ extra_args: list()\n\nchat = chat_ollama(model = \"llama3.2:3b\", seed = 111)\nchat$register_tool(tool_rnorm)\n\nFinally, we can now try out the tool calling ability of the LLM,\n\nchat$chat(\"Give me ten numbers from a random normal distribution with mean of 120 and standard deviation of 15.\")\n\n\nThe ten numbers generated from a normal distribution with a mean of 120 and \nstandard deviation of 15 are:\n\n1. 125.0977\n2. 140.8486\n3. 131.5419\n4. 140.3376\n5. 125.3349\n6. 134.1422\n7. 111.2048\n8. 126.6184\n9. 129.9178\n10. 123.316\n\n\nWe compare that to the response without tool calling using rollama’s query,\n\nrollama::query(\"Give me ten numbers from a random normal distribution with mean of 120 and standard deviation of 15.\",\n               model = \"llama3.2\", output = \"text\", screen = FALSE,\n               model_params = list(seed = 111)) |&gt; cat()\n\nHere are ten numbers randomly drawn from a normal distribution with a mean of 120 and a standard deviation of 15:\n\n1. 108\n2. 145\n3. 115\n4. 131\n5. 121\n6. 139\n7. 124\n8. 148\n9. 113\n10. 127\n\n\nIf you were to ask any LLM this simple question, “What’s the time now?”, you will be greeted with a standard example, i.e. “I don’t know” or its variants,\n\nrollama::query(\"What's the time now?\", \"llama3.2\",\n               output = \"text\", screen = FALSE,\n               model_params = list(seed = 111)) |&gt; cat()\n\nI'm not currently able to share the time.\n\n\nNow, we simplify the example from https://ellmer.tidyverse.org/articles/tool-calling.html by asking the current time.\n\n# Gets the current time in local PC time zone.\nget_current_time &lt;- function() {\n  Sys.time()\n}\nget_current_time()\n\n[1] \"2025-02-06 16:16:06 +08\"\n\n\nNow, we register the tool,\n\ntool_get_current_time = tool(\n  get_current_time,\n  \"Gets the current time in local PC.\")\ntool_get_current_time\n\n&lt;ellmer::ToolDef&gt;\n @ name       : chr \"get_current_time\"\n @ fun        : function ()  \n @ description: chr \"Gets the current time in local PC.\"\n @ arguments  : &lt;ellmer::TypeObject&gt;\n .. @ description          : NULL\n .. @ required             : logi TRUE\n .. @ properties           : list()\n .. @ additional_properties: logi FALSE\n\n\n\nProvider(base_url = \"http://localhost:11434/v1\")\n\n&lt;ellmer::Provider&gt;\n @ base_url  : chr \"http://localhost:11434/v1\"\n @ extra_args: list()\n\nchat = chat_ollama(model = \"llama3.2:3b\", seed = 123)\nchat$register_tool(tool_get_current_time)\n\nNote that for this step, so far I could not manage to get the code create_tool_metadata(get_current_time) running to automatically generate the metadata. This might change in the future version of ellmer.\nThen, lastly, we try the query,\n\nchat$chat(\"What's the time now?\")\n\n\nThe current time is 4:16 PM.\n\n\nwhich is the correct time in my PC at the time of writing.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous Techniques</span>"
    ]
  },
  {
    "objectID": "ch9.html#ai-agents",
    "href": "ch9.html#ai-agents",
    "title": "\n9  Miscellaneous Techniques\n",
    "section": "\n9.2 AI Agents",
    "text": "9.2 AI Agents\n\n9.2.1 About\nAn AI agent is an “AI model capable of reasoning, planning, and interacting with its environment” (\"Hugging Face\", 2025). It (i.e. the agent) reasons and plans, then takes actions by calling suitable tools, possibly in ways that we have discussed before i.e. tool calling.\nPotential academic uses:\n\nWeb search\nLR search and summary\nand more …\n\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous Techniques</span>"
    ]
  },
  {
    "objectID": "ch9.html#references",
    "href": "ch9.html#references",
    "title": "\n9  Miscellaneous Techniques\n",
    "section": "References",
    "text": "References\n\n\n\n\n\"Hugging Face\". (2025). What is an agent? Website. Retrieved from https://huggingface.co/learn/agents-course/en/unit1/what-are-agents",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Miscellaneous Techniques</span>"
    ]
  },
  {
    "objectID": "ch10.html",
    "href": "ch10.html",
    "title": "10  The Final Deep-dive",
    "section": "",
    "text": "In this chapter, I plan to demonstrate a basic LLM from scratch\n\ninternals of a Transformer model\nbuild decoder only Transformer model for a basic [your-name]-GPT model\nusing R torch – planned\n\nThis may rely heavily on https://github.com/rasbt/LLMs-from-scratch, in which I will simplify the steps in the book and adapt the code in R.\nIn progress …",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>The Final Deep-dive</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Aria, M. (2020). pubmedR: Gathering metadata about publications, grants, clinical trials from PubMed database. Retrieved from https://github.com/massimoaria/pubmedR\n\n\nAria, M., & Cuccurullo, C. (2017). Bibliometrix: An r-tool for comprehensive science mapping analysis. Journal of Informetrics. https://doi.org/10.1016/j.joi.2017.08.007\n\n\nAria, M., & Cuccurullo, C. (2025). Bibliometrix: Comprehensive science mapping analysis. Retrieved from https://www.bibliometrix.org\n\n\nCaballar, R. (2024). What are small language models? Website. Retrieved from https://www.ibm.com/think/topics/small-language-models\n\n\nGruber, J. B., & Weber, M. (2025). Rollama: Communicate with ollama to run large language models locally. Retrieved from https://jbgruber.github.io/rollama/\n\n\nHillier, D., Guertler, L., Tan, C., Agrawal, P., Ruirui, C., & Cheng, B. (2024). Super tiny language models. Retrieved from https://arxiv.org/abs/2405.14159\n\n\n\"Hugging Face\". (2025). What is an agent? Website. Retrieved from https://huggingface.co/learn/agents-course/en/unit1/what-are-agents\n\n\nIBM. (2024). What are large language models (LLMs)? Website. Retrieved from https://www.ibm.com/think/topics/large-language-models\n\n\nLin, H., & Safi, T. (2024). Ollamar: An r package for running large language models. PsyArXiv. https://doi.org/10.31234/osf.io/zsrg5\n\n\nLin, H., & Safi, T. (2025). Ollamar: Ollama language models. Retrieved from https://hauselin.github.io/ollama-r/\n\n\nMüller, K., & Wickham, H. (2023). Tibble: Simple data frames. Retrieved from https://tibble.tidyverse.org/\n\n\nR Core Team. (2025). R: A language and environment for statistical computing. Vienna, Austria: R Foundation for Statistical Computing. Retrieved from https://www.R-project.org/\n\n\nRuiz, E. (2024). Mall: Run multiple large language model predictions against a table, or vectors. Retrieved from https://mlverse.github.io/mall/\n\n\nSchoch, D., & Sax, C. (2025). Rchroma: A client for ChromaDB. Retrieved from https://github.com/cynkra/rchroma\n\n\nTang, Y., Liu, F., Ni, Y., Tian, Y., Bai, Z., Hu, Y.-Q., … Wang, Y. (2024). Rethinking optimization and architecture for tiny language models. Retrieved from https://arxiv.org/abs/2402.02791\n\n\n\"Unleashing the Power of Local LLMs\". (2024). Unleashing the power of local LLMs: A comprehensive guide. Website. Retrieved from https://localxpose.io/blog/unleashing-the-power-of-local-llms\n\n\nUrbanek, S. (2015). base64enc: Tools for base64 encoding. Retrieved from http://www.rforge.net/base64enc\n\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, & R. Garnett (Eds.), Advances in neural information processing systems (Vol. 30). Curran Associates, Inc. Retrieved from https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n\n\nWickham, H. (2023). Stringr: Simple, consistent wrappers for common string operations. Retrieved from https://stringr.tidyverse.org\n\n\nWickham, H., Cheng, J., Jacobs, A., & Aden-Buie, G. (2025). Ellmer: Chat with large language models. Retrieved from https://ellmer.tidyverse.org\n\n\nWickham, H., & Henry, L. (2025). Purrr: Functional programming tools. Retrieved from https://purrr.tidyverse.org/\n\n\nXie, Y. (2014). Knitr: A comprehensive tool for reproducible research in R. In V. Stodden, F. Leisch, & R. D. Peng (Eds.), Implementing reproducible computational research. Chapman; Hall/CRC.\n\n\nXie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Boca Raton, Florida: Chapman; Hall/CRC. Retrieved from https://yihui.org/knitr/\n\n\nXie, Y. (2025). Knitr: A general-purpose package for dynamic report generation in r. Retrieved from https://yihui.org/knitr/\n\n\nYang, A., Yang, B., Hui, B., Zheng, B., Yu, B., Zhou, C., … Fan, Z. (2024). Qwen2 technical report. arXiv Preprint arXiv:2407.10671.\n\n\nYang, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., … Qiu, Z. (2024). Qwen2.5 technical report. arXiv Preprint arXiv:2412.15115.",
    "crumbs": [
      "Bibliography"
    ]
  }
]