<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>2&nbsp; Local Large Language Models – Exploring Large Language Models With R</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./ch3.html" rel="next">
<link href="./ch1.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet">
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ch2.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Local Large Language Models</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Exploring Large Language Models With R</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Local Large Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Text Generation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Vision</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Embedding Generation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Text Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Retrieval Augmented Generation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Fine-tuning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Miscellaneous Techniques</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ch10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">The Final Deep-dive</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Bibliography</span></a>
  </div>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li>
<a href="#reasons-of-using-local-llms" id="toc-reasons-of-using-local-llms" class="nav-link" data-scroll-target="#reasons-of-using-local-llms"><span class="header-section-number">2.2</span> Reasons of Using Local LLMs</a>
  <ul class="collapse">
<li><a href="#exhibits" id="toc-exhibits" class="nav-link" data-scroll-target="#exhibits"><span class="header-section-number">2.2.1</span> Exhibits</a></li>
  <li><a href="#advantages" id="toc-advantages" class="nav-link" data-scroll-target="#advantages"><span class="header-section-number">2.2.2</span> Advantages</a></li>
  <li><a href="#personal-reasons" id="toc-personal-reasons" class="nav-link" data-scroll-target="#personal-reasons"><span class="header-section-number">2.2.3</span> Personal reasons</a></li>
  </ul>
</li>
  <li>
<a href="#using-local-llms" id="toc-using-local-llms" class="nav-link" data-scroll-target="#using-local-llms"><span class="header-section-number">2.3</span> Using Local LLMs</a>
  <ul class="collapse">
<li><a href="#hardware-requirements" id="toc-hardware-requirements" class="nav-link" data-scroll-target="#hardware-requirements"><span class="header-section-number">2.3.1</span> Hardware Requirements</a></li>
  <li><a href="#options-for-running-local-llms" id="toc-options-for-running-local-llms" class="nav-link" data-scroll-target="#options-for-running-local-llms"><span class="header-section-number">2.3.2</span> Options for Running Local LLMs</a></li>
  </ul>
</li>
  <li>
<a href="#running-local-llms-in-r" id="toc-running-local-llms-in-r" class="nav-link" data-scroll-target="#running-local-llms-in-r"><span class="header-section-number">2.4</span> Running Local LLMs in R</a>
  <ul class="collapse">
<li><a href="#ollama-meets-r" id="toc-ollama-meets-r" class="nav-link" data-scroll-target="#ollama-meets-r"><span class="header-section-number">2.4.1</span> Ollama Meets R</a></li>
  <li><a href="#getting-started-with-rollama" id="toc-getting-started-with-rollama" class="nav-link" data-scroll-target="#getting-started-with-rollama"><span class="header-section-number">2.4.2</span> Getting Started with <code>rollama</code></a></li>
  <li><a href="#basic-rollama-usage" id="toc-basic-rollama-usage" class="nav-link" data-scroll-target="#basic-rollama-usage"><span class="header-section-number">2.4.3</span> Basic <code>rollama</code> Usage</a></li>
  <li><a href="#llm-model-details" id="toc-llm-model-details" class="nav-link" data-scroll-target="#llm-model-details"><span class="header-section-number">2.4.4</span> LLM Model Details</a></li>
  <li><a href="#terms-to-understand" id="toc-terms-to-understand" class="nav-link" data-scroll-target="#terms-to-understand"><span class="header-section-number">2.4.5</span> Terms to understand</a></li>
  <li><a href="#pulling-additional-models-from-huggingface" id="toc-pulling-additional-models-from-huggingface" class="nav-link" data-scroll-target="#pulling-additional-models-from-huggingface"><span class="header-section-number">2.4.6</span> Pulling Additional Models from Huggingface</a></li>
  </ul>
</li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<h1 class="title">
<span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Local Large Language Models</span>
</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><section id="introduction" class="level2" data-number="2.1"><h2 data-number="2.1" class="anchored" data-anchor-id="introduction">
<span class="header-section-number">2.1</span> Introduction</h2>
<p>Local large language models (LLMs) “are versions of these powerful language models that run directly on a user’s device or local network, rather than in the cloud” <span class="citation" data-cites="localLLM2024">(<a href="#ref-localLLM2024" role="doc-biblioref">"Unleashing the Power of Local LLMs", 2024</a>)</span>. With the availability of relatively powerful consumer-grade GPUs with reasonably sized VRAM, we can run many open-weight LLM models locally. We will go through the reasons, requirements and options of running LLMs locally, and how to run it in R.</p>
</section><section id="reasons-of-using-local-llms" class="level2" data-number="2.2"><h2 data-number="2.2" class="anchored" data-anchor-id="reasons-of-using-local-llms">
<span class="header-section-number">2.2</span> Reasons of Using Local LLMs</h2>
<section id="exhibits" class="level3" data-number="2.2.1"><h3 data-number="2.2.1" class="anchored" data-anchor-id="exhibits">
<span class="header-section-number">2.2.1</span> Exhibits</h3>
<p>These are some of my main concerns about relying too much on any LLM services online, which can be illustrated clearly in these two pictures:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/gpt_down.png" class="img-fluid figure-img"></p>
<figcaption>Source: <a href="https://status.openai.com" class="uri">https://status.openai.com</a></figcaption></figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/gpt_limit.png" class="img-fluid figure-img"></p>
<figcaption>Source: <a href="https://chatgpt.com/" class="uri">https://chatgpt.com/</a> when you ran out of limit on free account.</figcaption></figure>
</div>
</section><section id="advantages" class="level3" data-number="2.2.2"><h3 data-number="2.2.2" class="anchored" data-anchor-id="advantages">
<span class="header-section-number">2.2.2</span> Advantages</h3>
<p>Local LLMs offer five key advantages <span class="citation" data-cites="localLLM2024">(<a href="#ref-localLLM2024" role="doc-biblioref">"Unleashing the Power of Local LLMs", 2024</a>)</span> that make them a compelling choice for various applications.</p>
<p><strong>Privacy</strong> is a major benefit, as these models process data entirely on your device, ensuring that sensitive information never leaves your control. Additionally, local LLMs provide <strong>reduced latency</strong> and <strong>offline</strong> functionality, making them ideal for scenarios where internet connectivity is unreliable or unnecessary. Without the need for constant cloud computing, users can enjoy faster response times and leverage AI capabilities even when disconnected from the web.</p>
<p>Using local LLMs is also <strong>cost-effective</strong>, as it eliminates ongoing expenses associated with cloud services, particularly for heavy users. Lastly, local deployment allows for greater <strong>customization</strong>, enabling fine-tuning of models to specific domains or use cases to meet precise needs.</p>
</section><section id="personal-reasons" class="level3" data-number="2.2.3"><h3 data-number="2.2.3" class="anchored" data-anchor-id="personal-reasons">
<span class="header-section-number">2.2.3</span> Personal reasons</h3>
<ul>
<li><p><strong>Privacy</strong>: As a university lecturer, some tasks are sensitive in nature, such as writing exam questions, brainstorming novel ideas, and drafting top-secret research. Therefore, privacy is crucial. Local LLMs ensure that all data remains on my device, keeping the data private.</p></li>
<li><p><strong>No Downtime</strong>: With local LLMs, I can work with them as long as my PC is running. I can work even if the internet goes down. This reliability ensures that I can focus entirely on my work.</p></li>
<li><p><strong>Experimentation</strong>: As a researcher, I love the freedom to experiment and iterate freely. Local LLMs provide this flexibility by allowing me to experiment with different models and explore different settings without worrying about whether I have reached my token limit of the day! As of today, local LLMs offer so much, and it is exciting to explore what they can do.</p></li>
</ul></section></section><section id="using-local-llms" class="level2" data-number="2.3"><h2 data-number="2.3" class="anchored" data-anchor-id="using-local-llms">
<span class="header-section-number">2.3</span> Using Local LLMs</h2>
<section id="hardware-requirements" class="level3" data-number="2.3.1"><h3 data-number="2.3.1" class="anchored" data-anchor-id="hardware-requirements">
<span class="header-section-number">2.3.1</span> Hardware Requirements</h3>
<p>For starter, you’ll need a gaming-specs PC/Laptop with an NVIDIA GPU.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/geforce-rtx-40-series-new.jpeg" class="img-fluid figure-img"></p>
<figcaption>Source: <a href="https://www.nvidia.com/en-us/geforce/graphics-cards/" class="uri">https://www.nvidia.com/en-us/geforce/graphics-cards/</a></figcaption></figure>
</div>
<p>As a disclaimer, I am not affiliated with NVIDIA, although I specifically mention NVIDIA GPUs here. As we will see later, packages/software for running local LLMs support NVIDIA GPUs, while the support for other GPUs may vary.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/Gemini_Generated_Image_4ro9b24ro9b24ro9.jpeg" class="img-fluid figure-img"></p>
<figcaption>Source: Generated with Gemini’s Imagen 3</figcaption></figure>
</div>
<p>And, of course, it is cool to have this one (i.e.&nbsp;a gaming PC) sitting on your desk for the sake of <em>research</em>. Admittedly, this is a gaming-specs PC, so it’s up to you what you want to do with it.</p>
</section><section id="options-for-running-local-llms" class="level3" data-number="2.3.2"><h3 data-number="2.3.2" class="anchored" data-anchor-id="options-for-running-local-llms">
<span class="header-section-number">2.3.2</span> Options for Running Local LLMs</h3>
<p>There are many options to run local LLMs, some of them are:</p>
<ul>
<li>Ollama (<a href="https://ollama.com/" class="uri">https://ollama.com/</a>) <svg aria-hidden="true" role="img" viewbox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"></path></svg>
</li>
<li>Ollama + Open WebUI (<a href="https://openwebui.com/" class="uri">https://openwebui.com/</a>) <svg aria-hidden="true" role="img" viewbox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"></path></svg>
</li>
<li>Msty (<a href="https://msty.app/" class="uri">https://msty.app/</a>)</li>
<li>LM Studio (<a href="https://lmstudio.ai/" class="uri">https://lmstudio.ai/</a>)</li>
<li>GPT4All (<a href="https://www.nomic.ai/gpt4all" class="uri">https://www.nomic.ai/gpt4all</a>)</li>
<li>vLLM (<a href="https://docs.vllm.ai/" class="uri">https://docs.vllm.ai/</a>)</li>
<li>llama.cpp (<a href="https://github.com/ggerganov/llama.cpp" class="uri">https://github.com/ggerganov/llama.cpp</a>) – essentially the originator of all listed above.</li>
</ul>
<p>In this book, we will use <strong>Ollama</strong> as the primary driver for running local LLMs and integrate it with our beloved R.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/ollama_main_page.png" class="img-fluid figure-img"></p>
<figcaption>Ollama main page <a href="https://ollama.com/" class="uri">https://ollama.com/</a></figcaption></figure>
</div>
<p>As it is, Ollama is run in CLI,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/ollama_cli.png" class="img-fluid figure-img"></p>
<figcaption>Ollama CLI shown running llama3.2</figcaption></figure>
</div>
<p>For daily use, Ollama is typically combined with a GUI, such as Open WebUI,</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/ollama_web_ui.png" class="img-fluid figure-img"></p>
<figcaption>Ollama in Open WebUI</figcaption></figure>
</div>
</section></section><section id="running-local-llms-in-r" class="level2" data-number="2.4"><h2 data-number="2.4" class="anchored" data-anchor-id="running-local-llms-in-r">
<span class="header-section-number">2.4</span> Running Local LLMs in R</h2>
<section id="ollama-meets-r" class="level3" data-number="2.4.1"><h3 data-number="2.4.1" class="anchored" data-anchor-id="ollama-meets-r">
<span class="header-section-number">2.4.1</span> Ollama Meets R</h3>
<p>There are several options that utilizes Ollama:</p>
<ul>
<li>
<code>rollama</code> <svg aria-hidden="true" role="img" viewbox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:red;overflow:visible;position:relative;"><path d="M47.6 300.4L228.3 469.1c7.5 7 17.4 10.9 27.7 10.9s20.2-3.9 27.7-10.9L464.4 300.4c30.4-28.3 47.6-68 47.6-109.5v-5.8c0-69.9-50.5-129.5-119.4-141C347 36.5 300.6 51.4 268 84L256 96 244 84c-32.6-32.6-79-47.5-124.6-39.9C50.5 55.6 0 115.2 0 185.1v5.8c0 41.5 17.2 81.2 47.6 109.5z"></path></svg> (<a href="https://jbgruber.github.io/rollama/" class="uri">https://jbgruber.github.io/rollama/</a>)</li>
<li>
<code>ollamar</code> (<a href="https://hauselin.github.io/ollama-r/" class="uri">https://hauselin.github.io/ollama-r/</a>)</li>
<li>
<code>mall</code> (<a href="https://mlverse.github.io/mall/" class="uri">https://mlverse.github.io/mall/</a>)</li>
</ul>
<p>There are also several other options that allow tapping into other LLM APIs other than Ollama:</p>
<ul>
<li>
<code>gptstudio</code> (<a href="https://michelnivard.github.io/gptstudio/" class="uri">https://michelnivard.github.io/gptstudio/</a>) and <code>gpttools</code> (<a href="https://jameshwade.github.io/gpttools/" class="uri">https://jameshwade.github.io/gpttools/</a>)</li>
<li>
<code>tidyllm</code> (<a href="https://edubruell.github.io/tidyllm/" class="uri">https://edubruell.github.io/tidyllm/</a>)</li>
</ul></section><section id="getting-started-with-rollama" class="level3" data-number="2.4.2"><h3 data-number="2.4.2" class="anchored" data-anchor-id="getting-started-with-rollama">
<span class="header-section-number">2.4.2</span> Getting Started with <code>rollama</code>
</h3>
<p>In this book, we will mainly rely on <code>rollama</code> package <span class="citation" data-cites="R-rollama">(<a href="#ref-R-rollama" role="doc-biblioref">Gruber &amp; Weber, 2024</a>)</span>. To get started, there are three basic preliminary steps:</p>
<ol type="1">
<li>Install Ollama <a href="https://ollama.com/download" class="uri">https://ollama.com/download</a> (this depends on the OS in your PC)</li>
<li>Install <code>rollama</code> package in R with <code>install.packages("rollama")</code>
</li>
<li>Find suitable models in <a href="https://ollama.com/search" class="uri">https://ollama.com/search</a>
</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/all.png" class="img-fluid figure-img"></p>
<figcaption>LLM models in ollama.com</figcaption></figure>
</div>
<p>Available LLM models in Ollama are divided by tasks that they perform:</p>
<ol type="1">
<li>
<strong>Text to Text</strong> – Text generation, i.e.&nbsp;text in / text out (a typical LLM)</li>
</ol>
<p>Examples: Llama (Meta, <a href="https://www.llama.com/" class="uri">https://www.llama.com/</a>), Qwen (Alibaba, <a href="https://qwenlm.github.io/" class="uri">https://qwenlm.github.io/</a>gemm), DeepSeek (DeepSeek, <a href="https://www.deepseek.com/" class="uri">https://www.deepseek.com/</a>), Gemma (Google, https://ai.google.dev/gemma), Mistral (Mistral AI, <a href="https://mistral.ai/" class="uri">https://mistral.ai/</a>) and Phi (Microsoft, <a href="https://azure.microsoft.com/en-us/products/phi" class="uri">https://azure.microsoft.com/en-us/products/phi</a>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/text.png" class="img-fluid figure-img"></p>
<figcaption>Text LLM / Tools in ollama.com</figcaption></figure>
</div>
<ol start="2" type="1">
<li>
<strong>Image + Text to Text</strong> – Vision LM, i.e.&nbsp;image + text in / text out</li>
</ol>
<p>Examples: LLaVA (<a href="https://llava-vl.github.io/" class="uri">https://llava-vl.github.io/</a>), Llama3.2-Vision (Meta, <a href="https://www.llama.com/" class="uri">https://www.llama.com/</a>), Moondream (Moondream AI, <a href="https://moondream.ai/" class="uri">https://moondream.ai/</a>)and MiniCPM-V (ModelBest, <a href="https://modelbest.cn/en" class="uri">https://modelbest.cn/en</a>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/vision.png" class="img-fluid figure-img"></p>
<figcaption>Vision LM in ollama.com</figcaption></figure>
</div>
<ol start="3" type="1">
<li>
<strong>Text to Number</strong> – Embedding generation, i.e.&nbsp;text in / numerical vector out</li>
</ol>
<p>Examples: Nomic Embed (Nomic AI <a href="https://www.nomic.ai/" class="uri">https://www.nomic.ai/</a>) and mxbai-embed-large (Mixedbread, <a href="https://www.mixedbread.ai/" class="uri">https://www.mixedbread.ai/</a>).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/text.png" class="img-fluid figure-img"></p>
<figcaption>Embedding models in ollama.com</figcaption></figure>
</div>
</section><section id="basic-rollama-usage" class="level3" data-number="2.4.3"><h3 data-number="2.4.3" class="anchored" data-anchor-id="basic-rollama-usage">
<span class="header-section-number">2.4.3</span> Basic <code>rollama</code> Usage</h3>
<p><strong>Install models</strong></p>
<p>You can pull models from Ollama with <code><a href="https://jbgruber.github.io/rollama/reference/pull_model.html">pull_model()</a></code>. Let’s pull <code>llama3.2</code>, <code>moondream</code>, and <code>nomic-embed-text</code> for starter,</p>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://jbgruber.github.io/rollama/">rollama</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/pull_model.html">pull_model</a></span><span class="op">(</span><span class="st">"llama3.2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/pull_model.html">pull_model</a></span><span class="op">(</span><span class="st">"moondream"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/pull_model.html">pull_model</a></span><span class="op">(</span><span class="st">"nomic-embed-text"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><strong>List installed Models</strong></p>
<p>You can list installed models in Ollama,</p>
<div class="cell">
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/list_models.html">list_models</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["name"],"name":[1],"type":["chr"],"align":["left"]},{"label":["model"],"name":[2],"type":["chr"],"align":["left"]},{"label":["modified_at"],"name":[3],"type":["chr"],"align":["left"]},{"label":["size"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["digest"],"name":[5],"type":["chr"],"align":["left"]},{"label":["parent_model"],"name":[6],"type":["chr"],"align":["left"]},{"label":["format"],"name":[7],"type":["chr"],"align":["left"]},{"label":["family"],"name":[8],"type":["chr"],"align":["left"]},{"label":["families"],"name":[9],"type":["list"],"align":["right"]},{"label":["parameter_size"],"name":[10],"type":["chr"],"align":["left"]},{"label":["quantization_level"],"name":[11],"type":["chr"],"align":["left"]}],"data":[{"1":"deepseek-r1:latest","2":"deepseek-r1:latest","3":"2025-01-24T23:36:52.00926415+08:00","4":"4683075271","5":"0a8c266910232fd3291e71e5ba1e058cc5af9d411192cf88b6d30e92b6e73163","6":"","7":"gguf","8":"qwen2","9":"<chr [1]>","10":"7.6B","11":"Q4_K_M"},{"1":"qwen2.5:latest","2":"qwen2.5:latest","3":"2025-01-18T15:51:08.418902321+08:00","4":"4683087332","5":"845dbda0ea48ed749caafd9e6037047aa19acfcfd82e704d7ca97d631a0b697e","6":"","7":"gguf","8":"qwen2","9":"<chr [1]>","10":"7.6B","11":"Q4_K_M"},{"1":"qwen2.5-coder:latest","2":"qwen2.5-coder:latest","3":"2025-01-04T15:42:47.574316104+08:00","4":"4683087519","5":"2b0496514337a3d5901f1d253d01726c890b721e891335a56d6e08cedf3e2cb0","6":"","7":"gguf","8":"qwen2","9":"<chr [1]>","10":"7.6B","11":"Q4_K_M"},{"1":"hf.co/RichardErkhov/mesolitica_-_malaysian-llama-3-8b-instruct-16k-gguf:Q4_K_M","2":"hf.co/RichardErkhov/mesolitica_-_malaysian-llama-3-8b-instruct-16k-gguf:Q4_K_M","3":"2025-01-04T00:00:38.829529791+08:00","4":"4920734653","5":"30e7364b96840abb96b2842755de5bf83edbf6989b2df17dfd2733ae2e7807d2","6":"","7":"gguf","8":"llama","9":"<chr [1]>","10":"8.03B","11":"unknown"},{"1":"moondream:latest","2":"moondream:latest","3":"2025-01-03T23:47:36.802347712+08:00","4":"1738451197","5":"55fc3abd386771e5b5d1bbcc732f3c3f4df6e9f9f08f1131f9cc27ba2d1eec5b","6":"","7":"gguf","8":"phi2","9":"<chr [1]>","10":"1B","11":"Q4_0"},{"1":"moondream:latest","2":"moondream:latest","3":"2025-01-03T23:47:36.802347712+08:00","4":"1738451197","5":"55fc3abd386771e5b5d1bbcc732f3c3f4df6e9f9f08f1131f9cc27ba2d1eec5b","6":"","7":"gguf","8":"phi2","9":"<chr [1]>","10":"1B","11":"Q4_0"},{"1":"llama3.2:latest","2":"llama3.2:latest","3":"2025-01-03T23:44:28.569611937+08:00","4":"2019393189","5":"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72","6":"","7":"gguf","8":"llama","9":"<chr [1]>","10":"3.2B","11":"Q4_K_M"},{"1":"nomic-embed-text:latest","2":"nomic-embed-text:latest","3":"2024-10-06T13:45:34.905642051+08:00","4":"274302450","5":"0a109f422b47e3a30ba2b10eca18548e944e8a23073ee3f3e947efcf3c45e59f","6":"","7":"gguf","8":"nomic-bert","9":"<chr [1]>","10":"137M","11":"F16"},{"1":"llama3.2:3b","2":"llama3.2:3b","3":"2024-10-02T11:14:27.051384633+08:00","4":"2019393189","5":"a80c4f17acd55265feec403c7aef86be0c25983ab279d83f3bcd3abbcb5b8b72","6":"","7":"gguf","8":"llama","9":"<chr [1]>","10":"3.2B","11":"Q4_K_M"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<p><strong>Query</strong></p>
<p><code><a href="https://jbgruber.github.io/rollama/reference/query.html">query()</a></code> is used when you want to ask a one-off query. The LLM will not remember the previous query.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">q</span> <span class="op">=</span> <span class="st">"Describe the history of R programming language"</span></span>
<span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/query.html">query</a></span><span class="op">(</span><span class="va">q</span>, <span class="st">"llama3.2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Answer from llama3.2 ────────────────────────────────────────────────────────</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The R programming language was created in the mid-1990s by Ross Ihaka and
Robert Gentleman at the University of Auckland, New Zealand. Here's a brief
history of R:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Early Days (1992-1995)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Ross Ihaka and Robert Gentleman were both statisticians and researchers who
wanted to create a software package for statistical analysis that was easy to
use, flexible, and accessible. They began working on the project in 1992 and
started calling it "S", after the programming language S-PLUS.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Release of S (1995)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The first release of S, version 3.0, was made available in 1995. This initial
version was a significant improvement over earlier versions but still had some
limitations.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Development of R (1997-2001)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 1997, Ihaka and Gentleman decided to create a new project called "R", which
stood for "Rehabilitation" or "Revitalization" of the S language. They wanted
to make the software more user-friendly and expand its capabilities.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Release of R 0.5 (1998)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The first release of R, version 0.5, was made available in 1998. This version
introduced several key features, including support for plotting, data
visualization, and statistical modeling.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Community Support (2001-2004)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 2001, the R project gained momentum with the creation of a mailing list,
which allowed users to ask questions, share ideas, and collaborate on
development. The R community grew rapidly, and by 2004, there were hundreds of
contributors to the project.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Release of R 2.0 (2005)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The release of R 2.0 in 2005 marked a significant milestone for the language.
This version introduced several new features, including support for
object-oriented programming, improved data visualization, and enhanced
statistical modeling capabilities.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**R Foundation Established (2006)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 2006, the R Foundation for Statistical Computing was established to manage
and coordinate the development of R. The foundation has played a crucial role
in promoting the language and providing support to users worldwide.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Release of R 3.0 (2012)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The release of R 3.0 in 2012 marked another major milestone for the language.
This version introduced several key features, including improved data
visualization, enhanced statistical modeling capabilities, and significant
performance improvements.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Current State (2020-Present)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Today, R is one of the most popular programming languages used in data science
and statistical analysis. The language has a thriving community of developers,
users, and contributors, with thousands of packages available for various
tasks, from data visualization to machine learning.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Overall, the history of R is marked by its commitment to simplicity,
flexibility, and accessibility. From its humble beginnings as S to its current
status as a leading data science language, R has become an indispensable tool
for statisticians, researchers, and data scientists around the world.</code></pre>
</div>
</div>
<p><strong>Chat</strong></p>
<p><code><a href="https://jbgruber.github.io/rollama/reference/query.html">chat()</a></code> is used when you want to ask several consecutive queries. The LLM will remember the previous queries. This is the behaviour of chat LLMs.</p>
<div class="cell">
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">q</span> <span class="op">=</span> <span class="st">"Describe the history of R programming language"</span></span>
<span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/query.html">chat</a></span><span class="op">(</span><span class="va">q</span>, <span class="st">"llama3.2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Answer from llama3.2 ────────────────────────────────────────────────────────</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The R programming language has a rich and fascinating history that spans over
three decades. Here's an overview:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Early Beginnings (1980s)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>R was first conceived in the early 1980s by Ross Ihaka, a statistician at the
University of Auckland in New Zealand. Ihaka began working on a statistical
analysis system for his own research needs, which eventually evolved into what
would become the R programming language.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**The First Release (1993)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 1993, Ihaka and Robert Gentleman, another statistician at Bell Labs in the
United States, released the first version of R, called "S-PLUS" or "R-1." This
initial release was based on the S programming language, which was developed by
John Chambers at AT&amp;T's Bell Labs.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**The Name Change (1995)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 1995, Ihaka and Gentleman decided to rename the system from "S-PLUS" to
simply "R," reportedly because they didn't want to associate it with the
commercial software S-PLUS.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**The R Development Team Forms (1996)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>As R gained popularity, a group of developers formed to continue its
development. The R Development Core Team was established in 1996, and it has
since been responsible for maintaining and updating the language.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**R 1.0 Released (2003)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In 2003, the first official version of R, R 1.0, was released. This marked a
significant milestone in the language's development, as it introduced many new
features and improvements.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**The CRAN (2002)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Around the same time, the Comprehensive R Archive Network (CRAN) was
established in 2002. CRAN is now the official repository for R packages, where
users can upload and share their own software components.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**R Packages Ecosystem (2000s)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In the mid-2000s, the R ecosystem began to grow rapidly, with the creation of
numerous package repositories like CRAN and Bioconductor. This led to a vast
array of tools and libraries available for data analysis and visualization in
R.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**Recent Developments (2010s-present)**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In recent years, R has continued to evolve with new features, such as:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>1. **Rcpp**: A bridge between R and C++, allowing developers to integrate C++
code into their R packages.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>2. **RShiny**: An interactive web application framework for creating
data-driven websites.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>3. **dplyr**: A popular package for efficient data manipulation and analysis.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>**The Future of R**</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Today, R is one of the most widely used programming languages in academia and
industry, particularly in the fields of statistics, data science, and machine
learning. The language's continued growth and development are driven by a
vibrant community of users and developers who contribute to its ecosystem
through new packages, plugins, and extensions.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Overall, the history of R is a story of collaborative innovation, with
contributions from many individuals over several decades. Its enduring
popularity is a testament to its flexibility, power, and versatility as a tool
for data analysis and visualization.</code></pre>
</div>
</div>
<p>Continue with the chat,</p>
<div class="cell">
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/query.html">chat</a></span><span class="op">(</span><span class="st">"Summarize it"</span>, <span class="st">"llama3.2"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Answer from llama3.2 ────────────────────────────────────────────────────────</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Here's a brief summary:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>R programming language was first conceived in the 1980s by Ross Ihaka at the
University of Auckland. The first version, R-1, was released in 1993, but it
wasn't until 2003 that the official version R 1.0 was released. In the
mid-2000s, the Comprehensive R Archive Network (CRAN) was established, and the
ecosystem began to grow rapidly.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>In recent years, R has continued to evolve with new features like Rcpp, RShiny,
and dplyr. Today, R is a widely used language in academia and industry,
particularly in statistics, data science, and machine learning. Its enduring
popularity is driven by a vibrant community of users and developers who
contribute to its ecosystem.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Key milestones:</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>* 1980s: Ross Ihaka conceives the idea for R</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>* 1993: First version of R released (R-1)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>* 2003: Official version R 1.0 released</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>* 2002: Comprehensive R Archive Network (CRAN) established</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>* Mid-2000s: R ecosystem grows rapidly with new package repositories and tools</code></pre>
</div>
</div>
</section><section id="llm-model-details" class="level3" data-number="2.4.4"><h3 data-number="2.4.4" class="anchored" data-anchor-id="llm-model-details">
<span class="header-section-number">2.4.4</span> LLM Model Details</h3>
<p><strong>View the details</strong></p>
<p>Important details of LLM models in your Ollama,</p>
<div class="cell">
<div class="sourceCode" id="cb107"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/list_models.html">list_models</a></span><span class="op">(</span><span class="op">)</span> <span class="op">|&gt;</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html">names</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "name"               "model"              "modified_at"       
 [4] "size"               "digest"             "parent_model"      
 [7] "format"             "family"             "families"          
[10] "parameter_size"     "quantization_level"</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb109"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://jbgruber.github.io/rollama/reference/list_models.html">list_models</a></span><span class="op">(</span><span class="op">)</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"name"</span>, <span class="st">"family"</span>, <span class="st">"parameter_size"</span>, <span class="st">"format"</span>, <span class="st">"quantization_level"</span><span class="op">)</span><span class="op">]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div data-pagedtable="false">
  <script data-pagedtable-source="" type="application/json">
{"columns":[{"label":["name"],"name":[1],"type":["chr"],"align":["left"]},{"label":["family"],"name":[2],"type":["chr"],"align":["left"]},{"label":["parameter_size"],"name":[3],"type":["chr"],"align":["left"]},{"label":["format"],"name":[4],"type":["chr"],"align":["left"]},{"label":["quantization_level"],"name":[5],"type":["chr"],"align":["left"]}],"data":[{"1":"deepseek-r1:latest","2":"qwen2","3":"7.6B","4":"gguf","5":"Q4_K_M"},{"1":"qwen2.5:latest","2":"qwen2","3":"7.6B","4":"gguf","5":"Q4_K_M"},{"1":"qwen2.5-coder:latest","2":"qwen2","3":"7.6B","4":"gguf","5":"Q4_K_M"},{"1":"hf.co/RichardErkhov/mesolitica_-_malaysian-llama-3-8b-instruct-16k-gguf:Q4_K_M","2":"llama","3":"8.03B","4":"gguf","5":"unknown"},{"1":"moondream:latest","2":"phi2","3":"1B","4":"gguf","5":"Q4_0"},{"1":"moondream:latest","2":"phi2","3":"1B","4":"gguf","5":"Q4_0"},{"1":"llama3.2:latest","2":"llama","3":"3.2B","4":"gguf","5":"Q4_K_M"},{"1":"nomic-embed-text:latest","2":"nomic-bert","3":"137M","4":"gguf","5":"F16"},{"1":"llama3.2:3b","2":"llama","3":"3.2B","4":"gguf","5":"Q4_K_M"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
</section><section id="terms-to-understand" class="level3" data-number="2.4.5"><h3 data-number="2.4.5" class="anchored" data-anchor-id="terms-to-understand">
<span class="header-section-number">2.4.5</span> Terms to understand</h3>
<p><strong>Model size</strong></p>
<ul>
<li>It usually described as the number of parameters in billions (B)</li>
<li>Parameters = weights in deep neural networks</li>
<li>Larger = better, but heavy to run (massive GPU requirement)</li>
<li>Smaller = (maybe) not as good, but lighter to run (consumer GPU can run)</li>
<li>Generally:
<ul>
<li>4B model = GPU 4Gb VRAM</li>
<li>8B model = GPU 8Gb VRAM</li>
<li>16B model = GPU 16Gb VRAM</li>
</ul>
</li>
</ul>
<p><strong>Quantization</strong></p>
<ul>
<li>It is a technique <em>“to reduce the model’s memory footprint and computational requirements without significantly sacrificing performance”</em>.</li>
<li>The process reduces the precision of the weights of LLM models.</li>
<li>Floating points: FP32, FP16; integers: Q2, Q4 (common, default in ollama), Q5, Q6, Q8. in bits, i.e.&nbsp;Q4 is 4-bit integer.</li>
<li>Quantization allows us to run local LLMs in our consumer grade PC.</li>
<li>More on quantization at <a href="https://huggingface.co/docs/hub/gguf#quantization-types" class="uri">https://huggingface.co/docs/hub/gguf#quantization-types</a>, and this Youtube video by Matt Williams that explains it very well <a href="https://youtu.be/K75j8MkwgJ0?si=W3KBSRJPlI0QpMxr" class="uri">https://youtu.be/K75j8MkwgJ0?si=W3KBSRJPlI0QpMxr</a>.</li>
</ul>
<p><strong>Context size</strong></p>
<ul>
<li>Context window/size is number of tokens (words or sub-words) that can be LLM can receive/produce as input/output. - It is around 3/2 times words in a given text.</li>
<li>You can try a context size calculator here: <a href="https://llmtokencounter.com/" class="uri">https://llmtokencounter.com/</a>
</li>
<li>This Youtube video by Matt Williams really explains the concept very well <a href="https://youtu.be/-Lyk7ygQw2E?si=RJwx9Xpl80MIDnuF" class="uri">https://youtu.be/-Lyk7ygQw2E?si=RJwx9Xpl80MIDnuF</a>.</li>
</ul></section><section id="pulling-additional-models-from-huggingface" class="level3" data-number="2.4.6"><h3 data-number="2.4.6" class="anchored" data-anchor-id="pulling-additional-models-from-huggingface">
<span class="header-section-number">2.4.6</span> Pulling Additional Models from Huggingface</h3>
<ul>
<li>Pull GGUF files from <a href="https://huggingface.co/models" class="uri">https://huggingface.co/models</a> for use in Ollama</li>
<li>You may find GGUF files for Malaysian LLM models (mostly fine-tuned and developed by mesolitica <a href="https://mesolitica.com/" class="uri">https://mesolitica.com/</a>)</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/mesolitica.png" class="img-fluid figure-img"></p>
<figcaption>mesolitica GGUF files</figcaption></figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="img/malaysian.png" class="img-fluid figure-img"></p>
<figcaption>malaysian GGUF files</figcaption></figure>
</div>
</section></section><section id="references" class="level2 unnumbered"><h2 class="unnumbered anchored" data-anchor-id="references">References</h2>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-R-rollama" class="csl-entry" role="listitem">
Gruber, J. B., &amp; Weber, M. (2024). <em>Rollama: Communicate with ollama to run large language models locally</em>. Retrieved from <a href="https://jbgruber.github.io/rollama/">https://jbgruber.github.io/rollama/</a>
</div>
<div id="ref-localLLM2024" class="csl-entry" role="listitem">
"Unleashing the Power of Local LLMs". (2024). Unleashing the power of local LLMs: A comprehensive guide. Website. Retrieved from <a href="https://localxpose.io/blog/unleashing-the-power-of-local-llms">https://localxpose.io/blog/unleashing-the-power-of-local-llms</a>
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/wnarifin\.github\.io\/explore_llm_r\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./ch1.html" class="pagination-link" aria-label="Large Language Models">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Large Language Models</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./ch3.html" class="pagination-link" aria-label="Text Generation">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Text Generation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>